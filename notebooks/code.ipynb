{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Python Version\n",
    "Check Your Python version before running this notebook.\n",
    "- Python 3.6.X is required to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use Python verison 3.6.x\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class PythonVersionException(Exception):\n",
    "    print('Please use Python verison 3.6.x')\n",
    "    pass;\n",
    "\n",
    "\n",
    "if re.match('3.6*', sys.version.split('(')[0]) is None:\n",
    "    raise PythonVersionException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Library imports\n",
    "Import all the library's required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NLTK Corpus Sets\n",
    "Run this section to check if the following corpus datasets have been downloaded, if they are missing this will download\n",
    "them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltkDataDir = '../data/nltk_data'\n",
    "\n",
    "nltk.data.path.append(os.path.abspath(nltkDataDir))\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('stopwords', nltkDataDir)\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"testing\")\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('punkt', nltkDataDir)\n",
    "    nltk.download('wordnet', nltkDataDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "\n",
    "\n",
    "def enron_selector():  # function to identify platform and selected dataset to be applied.\n",
    "    def init_lists(folder_collection, label):  # function to retrieve and apply email content to array.\n",
    "        a_list = []\n",
    "        doc_id = 0\n",
    "        file_list = []\n",
    "        label = \"Loading \" + label + \"...\"\n",
    "        for entry in folder_collection:\n",
    "            b_list = os.listdir(entry)\n",
    "            for item in b_list:\n",
    "                file_list.append(entry + item)\n",
    "        for a_file in file_list:\n",
    "            f = open(a_file, 'r')\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, file_list, label)\n",
    "            try:\n",
    "                a_list.append(f.read())\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            doc_id += 1\n",
    "        f.close()\n",
    "        return a_list\n",
    "\n",
    "    enron_ = ['Enron1/', 'Enron2/', 'Enron3/', 'Enron4/', 'Enron5/', 'Enron6/']\n",
    "    spam = []\n",
    "    ham = []\n",
    "\n",
    "    for i, sub in enumerate(enron_):\n",
    "        spam.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'spam/')\n",
    "        ham.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'ham/')\n",
    "\n",
    "    spam = init_lists(spam, \"spam\")\n",
    "    ham = init_lists(ham, \"ham\")\n",
    "    all_emails = [(email, 'spam') for email in spam]\n",
    "    all_emails += [(email, 'ham') for email in ham]\n",
    "    ham_emails, spam_emails = preprocess(all_emails)\n",
    "    ham_file, spam_file = \"../data/processed_ham.txt\", \"../data/processed_spam.txt\"\n",
    "    print(\"Writing ham file...\")\n",
    "    with open(ham_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in ham_emails))\n",
    "    print(\"Writing spam file...\")\n",
    "    with open(spam_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in spam_emails))\n",
    "    return ham_emails, spam_emails\n",
    "\n",
    "\n",
    "def test_collection(test_select):  # function that outlines all tests to be carried out.\n",
    "    if test_select == 1:\n",
    "        data_size = 1000 / 2\n",
    "    elif test_select == 2:\n",
    "        data_size = 2000 / 2\n",
    "    elif test_select == 3:\n",
    "        data_size = 3000 / 2\n",
    "    elif test_select == 4:\n",
    "        data_size = 1500 / 2\n",
    "    return data_size\n",
    "\n",
    "\n",
    "def preprocess(collection):  # function to apply pre-processing: stop words, lemmatise.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    label = 'Pre-processing emails...'\n",
    "    entry_id = 0\n",
    "    doc_id = 0\n",
    "    processed = []\n",
    "    for entry in collection:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, collection, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            emails = ''\n",
    "            if i == 0:\n",
    "                words = []\n",
    "                for word in word_tokenize(line):\n",
    "                    item = lemmatizer.lemmatize(word.lower())\n",
    "                    if not item in stoplist:\n",
    "                        if word.isalnum() == False:\n",
    "                            pass\n",
    "                        else:\n",
    "                            emails = emails + item + ','\n",
    "                processed.append(tuple((emails, entry[1])))\n",
    "                del emails\n",
    "                entry_id += 1\n",
    "        doc_id += 1\n",
    "    email_list = []\n",
    "    ham = []\n",
    "    spam = []\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'ham':\n",
    "            email_list.append(entry)\n",
    "            ham.append(entry)\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'spam':\n",
    "            email_list.append(entry)\n",
    "            spam.append(entry)\n",
    "    return ham, spam\n",
    "\n",
    "\n",
    "def dictionary_build(all_emails):\n",
    "    print(\"Building Dictionary...\")\n",
    "\n",
    "    def html_list():\n",
    "        html_tag_list = []\n",
    "        location = '../data/html_tag_list.txt'\n",
    "        f = open(location, 'r')\n",
    "        for i in f:\n",
    "            html_tag_list.append(i.strip())\n",
    "        f.close()\n",
    "        return html_tag_list\n",
    "\n",
    "    html_tags = [html_list()]\n",
    "    common_email_words = ['subject', 'mail', 'cc', '``', 'email', '\\n', 'www', 'com', '\\nsubject']\n",
    "    processed_emails = all_emails\n",
    "    all_words = []\n",
    "    for entry in all_emails:\n",
    "        for sentence in entry:\n",
    "            if not sentence == 'ham' or sentence == 'spam':\n",
    "                words = str(sentence).split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(word)\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = list(dictionary)\n",
    "    for item in list_to_remove:\n",
    "        if len(item) <= 1:\n",
    "            del dictionary[item]\n",
    "        elif item in html_tags:\n",
    "            del dictionary[item]\n",
    "        elif str(item).isdigit():\n",
    "            del dictionary[item]\n",
    "        elif item in common_email_words:\n",
    "            del dictionary[item]\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def extract_features(data,\n",
    "                     label):  # function to extract features to matrix based on calculating occurrence of words based\n",
    "    # on dictionary.\n",
    "    features_matrix = np.zeros((len(data), len(dictionary)))\n",
    "    label = 'Feature extraction \\'' + label + '\\':'\n",
    "    doc_id = 0\n",
    "    all_words = []\n",
    "    for entry in data:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, data, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            if i == 0:\n",
    "                # print('[' + str(doc_id) + '] ', entry)\n",
    "                words = line.split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(words)\n",
    "                    for j, d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            word_id = j\n",
    "                            features_matrix[doc_id, word_id] = words.count(word)\n",
    "        doc_id = doc_id + 1\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calculate(ham, spam):\n",
    "    main_proportion = 0.8\n",
    "    ham_size = int(len(ham) * main_proportion)\n",
    "    ham_train, ham_test = ham[:ham_size], ham[ham_size:]\n",
    "    spam_size = int(len(spam) * main_proportion)\n",
    "    spam_train, spam_test = spam[:spam_size], spam[spam_size:]\n",
    "    ham_train_size, spam_train_size = int(len(ham_train) * main_proportion), int(len(spam_train) * main_proportion)\n",
    "    ham_train, ham_train_dev = ham_train[:ham_train_size], ham_train[ham_train_size:]\n",
    "    spam_train, spam_train_dev = spam_train[:spam_train_size], spam_train[spam_train_size:]\n",
    "    train_set, train_dev_set, test_set = ham_train + spam_train, ham_train_dev + spam_train_dev, ham_test + spam_test\n",
    "    train_labels = np.zeros(len(train_set))\n",
    "    train_labels[(int((len(train_set)) - len(spam_train))):len(train_set)] = 1\n",
    "    train_dev_labels = np.zeros(len(train_dev_set))\n",
    "    train_dev_labels[(int((len(train_dev_set)) - len(spam_train_dev))):len(train_dev_set)] = 1\n",
    "    test_labels = np.zeros(len(test_set))\n",
    "    test_labels[(int((len(test_set)) - len(spam_test))):len(test_set)] = 1\n",
    "    print(\"Train set:\\n\", \"Ham: \", str(len(ham_train)), \"\\n\", \"Spam: \", str(len(spam_train)),\n",
    "          \"\\nTrain_Dev:\\n Ham:\", str(len(ham_train_dev)), \"\\n Spam:\", str(len(spam_train_dev)),\n",
    "          \"\\nTest set:\\n\", \"Ham: \", str(len(ham_test)), \"\\n\", \"Spam: \", str(len(spam_test)))\n",
    "    return train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels\n",
    "\n",
    "\n",
    "def load_(file, label):\n",
    "    if verbose_logs == False:\n",
    "        print('Loading ' + label + ' dataset')\n",
    "    with open(file, 'r') as fp:\n",
    "        values = []\n",
    "        doc_id = 0\n",
    "        size_file = fp.read().split(\";\")\n",
    "        for item in size_file:\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, size_file, \"loading \" + label + \" file...\")\n",
    "            values.append(item.split(\", \"))\n",
    "            doc_id += 1\n",
    "        return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_status(id, data, label):\n",
    "    if id + 1 < int(len(data)):\n",
    "        end_atp = \"\\r\"\n",
    "    elif id + 1 <= int(len(data)):\n",
    "        end_atp = \"\\n\"\n",
    "    return print(label, '%0.0i out of %0.0i: %0.0i' %\n",
    "                 (id + 1, len(data), int((id + 1) * (100 / len(data)))), '%', end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def roc_curve(X, y, model, cv, algorithm_name, i):\n",
    "    try:\n",
    "        # tprs = []\n",
    "        aucs = []\n",
    "        # mean_fpr = np.linspace(0, 1, 100)\n",
    "        for train, test in cv.split(X, y):\n",
    "            probas_ = model.fit(X[train], y[train]).predict_proba(X[test])\n",
    "            # Compute ROC curve and area the curve\n",
    "            fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "            # tprs.append(interp1d(mean_fpr, fpr, tpr))\n",
    "            # tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.5, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "            i += 1\n",
    "        # mean_tpr = np.mean(tprs, axis=0)\n",
    "        # mean_tpr[-1] = 1.0\n",
    "        # mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        # plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "        #          label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        #          lw=2, alpha=.8)\n",
    "        # std_tpr = np.std(tprs, axis=0)\n",
    "        # tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        # tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        # plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "        #                  label=r'$\\pm$ 1 std. dev.')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Development - ROC: ' + algorithm_name)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(\n",
    "            \"../results/plots/dev/devROC_%s_%0i_features_%0i_test.png\" % (\n",
    "                algorithm_name, feature_size, len(test_set)),\n",
    "            dpi=100,\n",
    "            facecolor='w', edgecolor='b', linewidth=1, orientation='portrait', papertype=None,\n",
    "            format=\"png\", transparent=False, bbox_inches=None, pad_inches=0.1, frameon=None)\n",
    "        print(\"Created %s ROC figure\" % (algorithm_name))\n",
    "        plt.close()\n",
    "    except (AttributeError, OverflowError) as detail:\n",
    "        print(algorithm_name + \" Failed due to \", detail)\n",
    "\n",
    "    return 1\n",
    "\n",
    "\n",
    "verbose_logs = False\n",
    "train_dev_features = []\n",
    "train_dev_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ham dataset\n",
      "Loading spam dataset\n",
      "Building Dictionary...\n",
      "[('enron', 60909), ('ect', 35672), ('company', 28725), ('please', 20344), ('ha', 20101), ('spam', 17907), ('wa', 17822), ('hou', 17264), ('would', 15531), ('new', 15268), ('time', 14848), ('price', 14224), ('business', 13582), ('may', 13139), ('information', 13117), ('one', 12342), ('gas', 11954), ('said', 11889), ('market', 11671), ('get', 11498), ('energy', 11463), ('year', 11415), ('http', 11175), ('day', 10853), ('need', 10847), ('message', 10769), ('stock', 10472), ('deal', 10058), ('know', 9782), ('pm', 9676), ('service', 9642), ('also', 9232), ('report', 9002), ('power', 8777), ('vince', 8655), ('security', 8651), ('thanks', 8432), ('week', 8372), ('like', 8289), ('statement', 7962), ('corp', 7954), ('make', 7938), ('number', 7841), ('million', 7762), ('inc', 7398), ('group', 7390), ('could', 7342), ('risk', 7182), ('sent', 7175), ('share', 7168)] \n",
      "\n",
      "Train set:\n",
      " Ham:  10588 \n",
      " Spam:  10978 \n",
      "Train_Dev:\n",
      " Ham: 2648 \n",
      " Spam: 2745 \n",
      "Test set:\n",
      " Ham:  3310 \n",
      " Spam:  3431\n",
      "Finished feature extraction\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_size = int(input(\"Type amount of features to use: \"))\n",
    "    model_process = str(input(\"Extract features or Test models? [Extract features = y, Test models = n\")) == 'y'\n",
    "    verbose_logs = str(input(\"Enable verbose logs? [y/n]\")) == 'y'\n",
    "    a_files = [\"../data/processed_ham.txt\", \"../data/processed_spam.txt\"]\n",
    "    a_exist = [f for f in a_files if os.path.isfile(f)]\n",
    "    usePreprocessedDatasets = False\n",
    "    if a_exist:\n",
    "        usePreprocessedDatasets = str(\n",
    "            input(\"Preprocessed datasets for ham and spam found, would you like to use them? [y/n]\")) == 'y'\n",
    "\n",
    "    if usePreprocessedDatasets:\n",
    "        ham_collection = load_(a_files[0], \"ham\")\n",
    "        spam_collection = load_(a_files[1], \"spam\")\n",
    "    else:\n",
    "        ham_collection, spam_collection = enron_selector()\n",
    "    dictionary = dictionary_build((ham_collection + spam_collection))\n",
    "    dictionary = dictionary.most_common(feature_size)\n",
    "    print(dictionary, \"\\n\")\n",
    "    train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels = calculate(ham_collection,\n",
    "                                                                                                spam_collection)\n",
    "    if model_process:\n",
    "        train_features = extract_features(train_set, \"train\")\n",
    "        train_dev_features = extract_features(train_dev_set, \"train_dev\")\n",
    "        test_features, test_labels = 0, 0\n",
    "    else:\n",
    "        test_features = extract_features(test_set, \"test\")\n",
    "        train_features, train_labels = 0, 0\n",
    "        train_dev_features, train_dev_labels = 0, 0\n",
    "\n",
    "    train_features_scaled = 0\n",
    "    train_dev_features_scaled = 0\n",
    "    test_features_scaled = 0\n",
    "\n",
    "    if isinstance(train_features, np.ndarray):\n",
    "        train_features_scaled = MinMaxScaler().fit_transform(train_features, train_labels)\n",
    "\n",
    "    if isinstance(train_dev_features, np.ndarray):\n",
    "        train_dev_features_scaled = MinMaxScaler().fit_transform(train_dev_features, train_dev_labels)\n",
    "\n",
    "    if isinstance(test_features, np.ndarray):\n",
    "        test_features_scaled = MinMaxScaler().fit_transform(test_features, test_labels)\n",
    "\n",
    "    print('Finished feature extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1349 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b98c9723cdb04222aad7a6fafc3ac846"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-51-526e5b41b2de>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m \u001B[0mshap_report\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_dev_features_scaled\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dev_labels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdictionary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-51-526e5b41b2de>\u001B[0m in \u001B[0;36mshap_report\u001B[1;34m(x, y, dictionary)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[1;31m# Calculate Shap values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m     \u001B[0mshap_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexplainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshap_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_X\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[0mshap\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msummary_plot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshap_values\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mformatted_dictionary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\development\\python\\expert systems\\venv\\lib\\site-packages\\shap\\explainers\\_kernel.py\u001B[0m in \u001B[0;36mshap_values\u001B[1;34m(self, X, **kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeep_index\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    185\u001B[0m                     \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_to_instance_with_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex_value\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mi\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 186\u001B[1;33m                 \u001B[0mexplanations\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    187\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    188\u001B[0m             \u001B[1;31m# vector-output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\development\\python\\expert systems\\venv\\lib\\site-packages\\shap\\explainers\\_kernel.py\u001B[0m in \u001B[0;36mexplain\u001B[1;34m(self, incoming_instance, **kwargs)\u001B[0m\n\u001B[0;32m    362\u001B[0m                         \u001B[1;32mif\u001B[0m \u001B[0mnew_sample\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m                             \u001B[0msamples_left\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 364\u001B[1;33m                             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maddsample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minstance\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    365\u001B[0m                         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    366\u001B[0m                             \u001B[1;31m# we know the compliment sample is the next one after the original sample, so + 1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\development\\python\\expert systems\\venv\\lib\\site-packages\\shap\\explainers\\_kernel.py\u001B[0m in \u001B[0;36maddsample\u001B[1;34m(self, x, m, w)\u001B[0m\n\u001B[0;32m    496\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0msp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0missparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0msp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msparse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0missparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msynth_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    497\u001B[0m                     \u001B[0mevaluation_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mevaluation_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 498\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msynth_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0moffset\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0moffset\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgroups\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mevaluation_data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    499\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmaskMatrix\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnsamplesAdded\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    500\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkernelWeights\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnsamplesAdded\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "def shap_report(x, y, dictionary):\n",
    "\n",
    "    formatted_dictionary = []\n",
    "    for item in dictionary:\n",
    "        formatted_dictionary.append(item[0])\n",
    "\n",
    "    X = pd.DataFrame(x, columns=formatted_dictionary)\n",
    "    y = pd.DataFrame(y)\n",
    "    \n",
    "    train_X, val_X, train_y, val_y = train_test_split(X, y.values.ravel(), random_state=1)\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=5, solver='lbfgs', max_iter=10000).fit(train_X, train_y)\n",
    "    # Create object that can calculate shap values\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, shap.sample(train_X, 100))\n",
    "\n",
    "    # Calculate Shap values\n",
    "    shap_values = explainer.shap_values(val_X)\n",
    "\n",
    "    shap.summary_plot(shap_values[1], None, formatted_dictionary, show=False)\n",
    "    plt.savefig('shap_plot.svg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "shap_report(train_dev_features_scaled, train_dev_labels, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def models(train_features, train_dev_features, test_features, train_labels, train_dev_labels, test_labels, model_process\n",
    "           , feature_size, test_set):\n",
    "    # function to hold classifiers, fit and prediction and finally report the performance based on return method.\n",
    "    algorithm_names = ['k-Neighbors Classifier',\n",
    "                       'MLP Neural Network 1',\n",
    "                       'MLP Neural Network 2',\n",
    "                       'Logistic Regression',\n",
    "                       'Random Forest',\n",
    "                       'xgBoost',\n",
    "                       'Multinomial Naive Bayes',\n",
    "                       'Gaussian NB',\n",
    "                       'Bernoulli NB',\n",
    "                       'Rbf SVC',\n",
    "                       'Linear SVC',\n",
    "                       'Poly SVC',\n",
    "                       'Sigmoid SVC']\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "\n",
    "    print('Processing models...')\n",
    "\n",
    "    model1 = KNeighborsClassifier(algorithm='brute')\n",
    "    model2 = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=25)\n",
    "    model3 = MLPClassifier(hidden_layer_sizes=(150, 150), solver='lbfgs', max_iter=25)\n",
    "    model4 = LogisticRegression(solver='lbfgs', max_iter=25)\n",
    "    model5 = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model6 = XGBClassifier()\n",
    "    model7 = MultinomialNB()\n",
    "    model8 = GaussianNB()\n",
    "    model9 = BernoulliNB()\n",
    "    model10 = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=200,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model11 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model12 = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=25,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model13 = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12,\n",
    "              model13]\n",
    "\n",
    "    if model_process:\n",
    "        process_id = int(input(\"Test or development? (0/1)\"))\n",
    "        if process_id == 0:\n",
    "            print(\"x-val train_set\")\n",
    "            score0 = cross_validate(model1, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score1 = cross_validate(model2, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score2 = cross_validate(model3, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score3 = cross_validate(model4, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score4 = cross_validate(model5, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score5 = cross_validate(model6, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score6 = cross_validate(model7, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score7 = cross_validate(model8, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score8 = cross_validate(model9, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score9 = cross_validate(model10, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score10 = cross_validate(model11, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                     return_train_score=False)\n",
    "            score11 = cross_validate(model12, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                     return_train_score=False)\n",
    "            scores = [score0, score1, score2, score3, score4, score5, score6, score7, score8, score9, score10, score11]\n",
    "            print('Train set model output...\\n')\n",
    "            for i, d in enumerate(scores):\n",
    "                print(algorithm_names[i])\n",
    "                for c, e in enumerate(scoring_parse_labels):\n",
    "                    item = d.pop(e)\n",
    "                    item = item.astype(np.float)\n",
    "                    if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                        print(scoring_parse_labels[c], ':', '%0.6f' % (np.mean(item)))\n",
    "                    else:\n",
    "                        print(scoring_parse_labels[c], ':', '%0.0f' % (float(np.mean(item) * 100)) + '%')\n",
    "                        if scoring_parse_labels[c] == 'test_f1':\n",
    "                            print('\\n')\n",
    "        elif process_id == 1:\n",
    "            print(\"ROC curve development\")\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "            X, y = train_dev_features, train_dev_labels\n",
    "            k = 0\n",
    "            save_output = str(input('model record output (y/n): ')) == 'y'\n",
    "            scores = []\n",
    "            for model in models:\n",
    "                i = 0\n",
    "                if save_output:\n",
    "                    k != roc_curve(X, y, model, cv, algorithm_names[k], i)\n",
    "                else:\n",
    "                    scores.append(cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False))\n",
    "\n",
    "            for i, d in enumerate(scores):\n",
    "                print(algorithm_names[i])\n",
    "                for c, e in enumerate(scoring_parse_labels):\n",
    "                    item = d.pop(e)\n",
    "                    item = item.astype(np.float)\n",
    "                    if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                        print(scoring_parse_labels[c], ':', '%0.6f' % (np.mean(item)))\n",
    "                    else:\n",
    "                        print(scoring_parse_labels[c], ':', '%0.0f' % (float(np.mean(item) * 100)) + '%')\n",
    "                        if scoring_parse_labels[c] == 'test_f1':\n",
    "                            print('\\n')\n",
    "\n",
    "    else:\n",
    "        print(\"ROC Curve output\")\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "        X, y = test_features, test_labels\n",
    "        k = 0\n",
    "        for model in models:\n",
    "            i = 0\n",
    "            k != roc_curve(X, y, model, cv, algorithm_names[k], i)\n",
    "\n",
    "\n",
    "models(train_features, train_dev_features, test_features, train_labels, train_dev_labels, test_labels, model_process,\n",
    "       feature_size, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_test(train, test):  # function to test and record via csv, all algorithms selected.\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    model1 = KNeighborsClassifier(algorithm='brute')\n",
    "    model2 = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=25)\n",
    "    model3 = MLPClassifier(hidden_layer_sizes=(150, 150), solver='lbfgs', max_iter=25)\n",
    "    model4 = LogisticRegression(solver='lbfgs', max_iter=25)\n",
    "    model5 = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model6 = XGBClassifier()\n",
    "    model7 = MultinomialNB()\n",
    "    model8 = GaussianNB()\n",
    "    model9 = BernoulliNB()\n",
    "    model10 = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=200,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model11 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model12 = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=25,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model13 = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12,\n",
    "              model13]\n",
    "    for a in range(len(models)):\n",
    "        scores.append(cross_validate(models[a], train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "    with open('../results/feature_test_1.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "feature_test(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def svm_test(X, y):  # function test for SVM models specified.\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X, y)\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    max_iter = [10, 25, 50, 75, 100, 150, 200, 2000, 5000, 8000, 10000, 15000, 20000, 50000, 100000]\n",
    "    for i in range(len(max_iter)):\n",
    "        print(\"iterations: \" + str(max_iter[i]))\n",
    "        model = svm.LinearSVC(max_iter=max_iter[i])\n",
    "        scores.append(cross_validate(model, x_scaled, y, cv=cv, scoring=scoring, return_train_score=False))\n",
    "\n",
    "    with open('../results/svm_svc_test.csv', 'w', newline='') as csvfile:  # function to save output to csv file.\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "svm_test(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mpnn_test(train, test):  # Multi-layer perceptron neural network test.\n",
    "    nu_val = [10, 25, 50, 75, 100, 150, 200]\n",
    "    h_layers = [1, 2, 3, 4, 5]\n",
    "    iter_val = [10, 25, 50, 100, 200]\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    for a in range(len(nu_val)):\n",
    "        for b in range(len(h_layers)):\n",
    "            if b > 0:\n",
    "                if b == 1:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a]\n",
    "                elif b == 2:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 3:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 4:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "            else:\n",
    "                nu_layer_val = nu_val[a]\n",
    "            for c in range(len(iter_val)):\n",
    "                print('%0.0i out of %0.0i/ %0.0i' %\n",
    "                      (i, (int(len(nu_val) * len(h_layers) * len(iter_val))),\n",
    "                       int(i * (100 / (int(len(nu_val) * len(h_layers) * len(iter_val)))))) + '%',\n",
    "                      end='\\r', flush=True)\n",
    "                model = MLPClassifier(hidden_layer_sizes=(nu_layer_val), solver='lbfgs', max_iter=iter_val[c])\n",
    "                scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "                i += 1\n",
    "    with open('../results/mpnn_test.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "mpnn_test(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_2(train, test):\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(train)\n",
    "    i = 0\n",
    "    kernal_val = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    iter_val = [10, 25, 50, 100, 200]\n",
    "    for a in range(len(kernal_val)):\n",
    "        for b in range(len(iter_val)):\n",
    "            print('%0.0i out of %0.0i/ %0.0i' %\n",
    "                  (i, (int(len(kernal_val) * len(iter_val))), int(i * (100 / (int(len(kernal_val) * len(iter_val))))))\n",
    "                  + '%', end='\\r', flush=True)\n",
    "            model = svm.SVC(C=1.0, kernel=kernal_val[a], degree=3, gamma='auto', coef0=0.0, shrinking=True,\n",
    "                            probability=True, tol=0.001, cache_size=10000, class_weight=None, verbose=False,\n",
    "                            max_iter=iter_val[b], decision_function_shape='ovr', random_state=None)\n",
    "            scores.append(cross_validate(model, scaled_data, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "            i += 1\n",
    "    with open('../results/test_2.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "test_2(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_3(train, test):  # todo Knn classifier test.\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    algorithm_val = ['ball_tree', 'kd_tree', 'brute']\n",
    "    for a in range(len(algorithm_val)):\n",
    "        model = KNeighborsClassifier(algorithm=algorithm_val[a])\n",
    "        scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "        i += 1\n",
    "    with open('../results/test_3.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "test_3(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_4(train, test):\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    model = LogisticRegression()\n",
    "    scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "    i += 1\n",
    "    with open('../results/test_4.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "test_4(train_dev_features, train_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# todo: implement t-test with Bonferroni correction, if not can this be done in excel with current results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# todo: take best models and remove some of the pre-processing stages e.g. lemmatize, stop work and html tag removal,\n",
    "# provide scores and potentially roc curves too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# todo: implement confusion matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}