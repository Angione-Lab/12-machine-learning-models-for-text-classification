{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Python Version\n",
    "Check Your Python version before running this notebook.\n",
    "- Python 3.6.X is required to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class PythonVersionException(Exception):\n",
    "    pass;\n",
    "\n",
    "\n",
    "if re.match('3.6*', sys.version.split('(')[0]) is None:\n",
    "    print(sys.version.split('(')[0], ' Please use Python 3.6.X')\n",
    "    raise PythonVersionException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Library imports\n",
    "Import all the library's required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NLTK Corpus Sets\n",
    "Run this section to check if the following corpus datasets have been downloaded, if they are missing this will download\n",
    "them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltkDataDir = '../data/nltk_data'\n",
    "\n",
    "nltk.data.path.append(os.path.abspath(nltkDataDir))\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('stopwords', nltkDataDir)\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"testing\")\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('punkt', nltkDataDir)\n",
    "    nltk.download('wordnet', nltkDataDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "\n",
    "\n",
    "def enron_selector():  # function to identify platform and selected dataset to be applied.\n",
    "    def init_lists(folder_collection, label):  # function to retrieve and apply email content to array.\n",
    "        a_list = []\n",
    "        doc_id = 0\n",
    "        file_list = []\n",
    "        label = \"Loading \" + label + \"...\"\n",
    "        for entry in folder_collection:\n",
    "            b_list = os.listdir(entry)\n",
    "            for item in b_list:\n",
    "                file_list.append(entry + item)\n",
    "        for a_file in file_list:\n",
    "            f = open(a_file, 'r')\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, file_list, label)\n",
    "            try:\n",
    "                a_list.append(f.read())\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            doc_id += 1\n",
    "        f.close()\n",
    "        return a_list\n",
    "\n",
    "    enron_ = ['Enron1/', 'Enron2/', 'Enron3/', 'Enron4/', 'Enron5/', 'Enron6/']\n",
    "    spam = []\n",
    "    ham = []\n",
    "\n",
    "    for i, sub in enumerate(enron_):\n",
    "        spam.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'spam/')\n",
    "        ham.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'ham/')\n",
    "\n",
    "    spam = init_lists(spam, \"spam\")\n",
    "    ham = init_lists(ham, \"ham\")\n",
    "    all_emails = [(email, 'spam') for email in spam]\n",
    "    all_emails += [(email, 'ham') for email in ham]\n",
    "    ham_emails, spam_emails = preprocess(all_emails)\n",
    "    ham_file, spam_file = \"../data/processed_ham.txt\", \"../data/processed_spam.txt\"\n",
    "    print(\"Writing ham file...\")\n",
    "    with open(ham_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in ham_emails))\n",
    "    print(\"Writing spam file...\")\n",
    "    with open(spam_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in spam_emails))\n",
    "    return ham_emails, spam_emails\n",
    "\n",
    "\n",
    "def test_collection(test_select):  # function that outlines all tests to be carried out.\n",
    "    if test_select == 1:\n",
    "        data_size = 1000 / 2\n",
    "    elif test_select == 2:\n",
    "        data_size = 2000 / 2\n",
    "    elif test_select == 3:\n",
    "        data_size = 3000 / 2\n",
    "    elif test_select == 4:\n",
    "        data_size = 1500 / 2\n",
    "    return data_size\n",
    "\n",
    "\n",
    "def preprocess(collection):  # function to apply pre-processing: stop words, lemmatise.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    label = 'Pre-processing emails...'\n",
    "    entry_id = 0\n",
    "    doc_id = 0\n",
    "    processed = []\n",
    "    for entry in collection:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, collection, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            emails = ''\n",
    "            if i == 0:\n",
    "                words = []\n",
    "                for word in word_tokenize(line):\n",
    "                    item = lemmatizer.lemmatize(word.lower())\n",
    "                    if not item in stoplist:\n",
    "                        if word.isalnum() == False:\n",
    "                            pass\n",
    "                        else:\n",
    "                            emails = emails + item + ','\n",
    "                processed.append(tuple((emails, entry[1])))\n",
    "                del emails\n",
    "                entry_id += 1\n",
    "        doc_id += 1\n",
    "    email_list = []\n",
    "    ham = []\n",
    "    spam = []\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'ham':\n",
    "            email_list.append(entry)\n",
    "            ham.append(entry)\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'spam':\n",
    "            email_list.append(entry)\n",
    "            spam.append(entry)\n",
    "    return ham, spam\n",
    "\n",
    "\n",
    "def make_dictionary(all_emails):  # function to create dictionary, removing html tags, non alpha items e.g. numbers.\n",
    "    def html_list():\n",
    "        html_tag_list = []\n",
    "        location = '../data/html_tag_list.txt'\n",
    "        f = open(location, 'r')\n",
    "        for i in f:\n",
    "            html_tag_list.append(i.strip())\n",
    "        f.close()\n",
    "        return html_tag_list\n",
    "\n",
    "    html_tags = [html_list()]\n",
    "    processed_emails = all_emails\n",
    "    all_words = []\n",
    "    for entry in all_emails:\n",
    "        for sentence in entry:\n",
    "            if not sentence == 'ham' or sentence == 'spam':\n",
    "                words = str(sentence).split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(word)\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = list(dictionary)\n",
    "    for item in list_to_remove:\n",
    "        if len(item) <= 1:\n",
    "            del dictionary[item]\n",
    "        elif item in html_tags:\n",
    "            del dictionary[item]\n",
    "        elif str(item).isdigit():\n",
    "            del dictionary[item]\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def dictionary_build(emails):\n",
    "    print(\"Building Dictionary...\")\n",
    "    feature_dictionary = make_dictionary(emails)\n",
    "    return feature_dictionary\n",
    "\n",
    "\n",
    "def extract_features(data,\n",
    "                     label):  # function to extract features to matrix based on calculating occurrence of words based\n",
    "    # on dictionary.\n",
    "    features_matrix = np.zeros((len(data), len(dictionary)))\n",
    "    label = 'Feature extraction \\'' + label + '\\':'\n",
    "    doc_id = 0\n",
    "    all_words = []\n",
    "    for entry in data:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, data, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            if i == 0:\n",
    "                # print('[' + str(doc_id) + '] ', entry)\n",
    "                words = line.split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(words)\n",
    "                    for j, d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            word_id = j\n",
    "                            features_matrix[doc_id, word_id] = words.count(word)\n",
    "        doc_id = doc_id + 1\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calculate(ham, spam):\n",
    "    main_proportion = 0.8\n",
    "    ham_size = int(len(ham) * main_proportion)\n",
    "    ham_train, ham_test = ham[:ham_size], ham[ham_size:]\n",
    "    spam_size = int(len(spam) * main_proportion)\n",
    "    spam_train, spam_test = spam[:spam_size], spam[spam_size:]\n",
    "    ham_train_size, spam_train_size = int(len(ham_train) * main_proportion), int(len(spam_train) * main_proportion)\n",
    "    ham_train, ham_train_dev = ham_train[:ham_train_size], ham_train[ham_train_size:]\n",
    "    spam_train, spam_train_dev = spam_train[:spam_train_size], spam_train[spam_train_size:]\n",
    "    train_set, train_dev_set, test_set = ham_train + spam_train, ham_train_dev + spam_train_dev, ham_test + spam_test\n",
    "    train_labels = np.zeros(len(train_set))\n",
    "    train_labels[(int((len(train_set)) - len(spam_train))):len(train_set)] = 1\n",
    "    train_dev_labels = np.zeros(len(train_dev_set))\n",
    "    train_dev_labels[(int((len(train_dev_set)) - len(spam_train_dev))):len(train_dev_set)] = 1\n",
    "    test_labels = np.zeros(len(test_set))\n",
    "    test_labels[(int((len(test_set)) - len(spam_test))):len(test_set)] = 1\n",
    "    print(\"Train set:\\n\", \"Ham: \", str(len(ham_train)), \"\\n\", \"Spam: \", str(len(spam_train)),\n",
    "          \"\\nTrain_Dev:\\n Ham:\", str(len(ham_train_dev)), \"\\n Spam:\", str(len(spam_train_dev)),\n",
    "          \"\\nTest set:\\n\", \"Ham: \", str(len(ham_test)), \"\\n\", \"Spam: \", str(len(spam_test)))\n",
    "    return train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels\n",
    "\n",
    "\n",
    "def load_(file, label):\n",
    "    if verbose_logs == False:\n",
    "        print('Loading ' + label + ' dataset')\n",
    "    with open(file, 'r') as fp:\n",
    "        values = []\n",
    "        doc_id = 0\n",
    "        size_file = fp.read().split(\";\")\n",
    "        for item in size_file:\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, size_file, \"loading \" + label + \" file...\")\n",
    "            values.append(item.split(\", \"))\n",
    "            doc_id += 1\n",
    "        return values\n",
    "\n",
    "\n",
    "def feature_test(train, test):  # function to test and record via csv, all algorithms selected.\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    model1 = KNeighborsClassifier(algorithm='brute')\n",
    "    model2 = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=25)\n",
    "    model3 = MLPClassifier(hidden_layer_sizes=(150, 150), solver='lbfgs', max_iter=25)\n",
    "    model4 = LogisticRegression(solver='lbfgs', max_iter=25)\n",
    "    model5 = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model6 = XGBClassifier()\n",
    "    model7 = MultinomialNB()\n",
    "    model8 = GaussianNB()\n",
    "    model9 = BernoulliNB()\n",
    "    model10 = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=200,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model11 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model12 = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=25,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model13 = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12,\n",
    "              model13]\n",
    "    for a in range(len(models)):\n",
    "        scores.append(cross_validate(models[a], train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "    with open('../results/feature_test_1.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "\n",
    "            filewriter.writerow(processed_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_status(id, data, label):\n",
    "    if id + 1 < int(len(data)):\n",
    "        end_atp = \"\\r\"\n",
    "    elif id + 1 <= int(len(data)):\n",
    "        end_atp = \"\\n\"\n",
    "    return print(label, '%0.0i out of %0.0i: %0.0i' %\n",
    "                 (id + 1, len(data), int((id + 1) * (100 / len(data)))), '%', end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ham dataset\n",
      "Loading spam dataset\n",
      "Building Dictionary...\n",
      "[('enron', 60909), ('ect', 35672), ('\\nsubject', 33696), ('company', 28725), ('com', 24159), ('please', 20344), ('ha', 20101), ('spam', 17907), ('wa', 17822), ('hou', 17264), ('would', 15531), ('new', 15268), ('time', 14848), ('price', 14224), ('subject', 14157), ('business', 13582), ('may', 13139), ('information', 13117), ('one', 12342), ('gas', 11954), ('said', 11889), ('market', 11671), ('get', 11498), ('energy', 11463), ('year', 11415), ('http', 11175), ('email', 11012), ('day', 10853), ('need', 10847), ('message', 10769), ('stock', 10472), ('deal', 10058), ('know', 9782), ('pm', 9676), ('service', 9642), ('mail', 9563), ('cc', 9391), ('also', 9232), ('report', 9002), ('power', 8777), ('vince', 8655), ('security', 8651), ('thanks', 8432), ('week', 8372), ('like', 8289), ('statement', 7962), ('corp', 7954), ('make', 7938), ('number', 7841), ('million', 7762), ('www', 7730), ('inc', 7398), ('group', 7390), ('could', 7342), ('risk', 7182), ('sent', 7175), ('share', 7168), ('product', 7075), ('trading', 6984), ('investment', 6953), ('money', 6862), ('see', 6784), ('work', 6718), ('system', 6681), ('want', 6425), ('let', 6383), ('forward', 6375), ('call', 6352), ('order', 6331), ('contact', 6281), ('month', 6259), ('de', 6188), ('free', 6165), ('financial', 6154), ('within', 6090), ('next', 6072), ('last', 6019), ('term', 5955), ('go', 5933), ('credit', 5927), ('houston', 5868), ('take', 5862), ('name', 5850), ('offer', 5722), ('change', 5704), ('state', 5634), ('best', 5628), ('list', 5582), ('today', 5575), ('date', 5573), ('question', 5562), ('meeting', 5544), ('use', 5531), ('address', 5511), ('management', 5506), ('news', 5449), ('project', 5388), ('two', 5386), ('account', 5354), ('billion', 5315), ('right', 5262), ('customer', 5177), ('future', 5144), ('based', 5124), ('site', 5116), ('th', 5092), ('office', 5090), ('first', 5060), ('investor', 5053), ('sale', 5033), ('well', 5029), ('people', 4986), ('original', 4961), ('program', 4940), ('contract', 4934), ('transaction', 4899), ('data', 4874), ('dynegy', 4856), ('kaminski', 4809), ('software', 4770), ('help', 4766), ('many', 4740), ('click', 4732), ('per', 4686), ('net', 4679), ('made', 4655), ('back', 4617), ('issue', 4563), ('result', 4557), ('online', 4499), ('plan', 4487), ('looking', 4469), ('cost', 4467), ('look', 4463), ('way', 4410), ('send', 4407), ('hour', 4398), ('rate', 4394), ('good', 4380), ('available', 4360), ('attached', 4359), ('bank', 4321), ('line', 4224), ('interest', 4223), ('mr', 4206), ('following', 4196), ('schedule', 4136), ('regard', 4134), ('home', 4124), ('much', 4106), ('research', 4053), ('start', 4004), ('process', 3974), ('part', 3970), ('john', 3947), ('position', 3939), ('review', 3939), ('fund', 3928), ('due', 3895), ('jones', 3886), ('operation', 3881), ('high', 3865), ('forwarded', 3865), ('note', 3852), ('development', 3845), ('request', 3796), ('long', 3792), ('asset', 3762), ('california', 3740), ('phone', 3732), ('provide', 3715), ('team', 3711), ('since', 3702), ('think', 3684), ('louise', 3671), ('say', 3664), ('cash', 3645), ('thank', 3641), ('find', 3616), ('file', 3589), ('doe', 3585), ('friday', 3536), ('current', 3523), ('internet', 3516), ('value', 3510), ('buy', 3504), ('option', 3502), ('agreement', 3501), ('world', 3492), ('event', 3490), ('give', 3487), ('website', 3470), ('come', 3441), ('trade', 3414), ('dow', 3400), ('monday', 3391), ('international', 3352), ('employee', 3341), ('industry', 3333), ('act', 3326)] \n",
      "\n",
      "Train set:\n",
      " Ham:  10588 \n",
      " Spam:  10978 \n",
      "Train_Dev:\n",
      " Ham: 2648 \n",
      " Spam: 2745 \n",
      "Test set:\n",
      " Ham:  3310 \n",
      " Spam:  3431\n",
      "Processing models...\n",
      "x-val train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:21:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:21:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:21:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:21:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set model output...\n",
      "\n",
      "k-Neighbors Classifier\n",
      "fit_time : 0.022499\n",
      "score_time : 1.640196\n",
      "test_precision_macro : 91%\n",
      "test_recall_macro : 90%\n",
      "test_f1 : 91%\n",
      "\n",
      "\n",
      "MLP Neural Network 1\n",
      "fit_time : 2.047501\n",
      "score_time : 0.015899\n",
      "test_precision_macro : 96%\n",
      "test_recall_macro : 95%\n",
      "test_f1 : 96%\n",
      "\n",
      "\n",
      "MLP Neural Network 2\n",
      "fit_time : 6.296401\n",
      "score_time : 0.026600\n",
      "test_precision_macro : 96%\n",
      "test_recall_macro : 96%\n",
      "test_f1 : 96%\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "fit_time : 0.146201\n",
      "score_time : 0.008798\n",
      "test_precision_macro : 95%\n",
      "test_recall_macro : 94%\n",
      "test_f1 : 95%\n",
      "\n",
      "\n",
      "Random Forest\n",
      "fit_time : 2.573300\n",
      "score_time : 0.065806\n",
      "test_precision_macro : 96%\n",
      "test_recall_macro : 96%\n",
      "test_f1 : 96%\n",
      "\n",
      "\n",
      "xgBoost\n",
      "fit_time : 3.293400\n",
      "score_time : 0.036301\n",
      "test_precision_macro : 96%\n",
      "test_recall_macro : 96%\n",
      "test_f1 : 96%\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "fit_time : 0.028600\n",
      "score_time : 0.010100\n",
      "test_precision_macro : 93%\n",
      "test_recall_macro : 93%\n",
      "test_f1 : 93%\n",
      "\n",
      "\n",
      "Gaussian NB\n",
      "fit_time : 0.075999\n",
      "score_time : 0.027409\n",
      "test_precision_macro : 88%\n",
      "test_recall_macro : 86%\n",
      "test_f1 : 88%\n",
      "\n",
      "\n",
      "Bernoulli NB\n",
      "fit_time : 0.060592\n",
      "score_time : 0.018097\n",
      "test_precision_macro : 90%\n",
      "test_recall_macro : 88%\n",
      "test_f1 : 90%\n",
      "\n",
      "\n",
      "Rbf SVC\n",
      "fit_time : 12.671986\n",
      "score_time : 0.833709\n",
      "test_precision_macro : 64%\n",
      "test_recall_macro : 62%\n",
      "test_f1 : 69%\n",
      "\n",
      "\n",
      "Linear SVC\n",
      "fit_time : 3.341503\n",
      "score_time : 0.054197\n",
      "test_precision_macro : 79%\n",
      "test_recall_macro : 70%\n",
      "test_f1 : 77%\n",
      "\n",
      "\n",
      "Poly SVC\n",
      "fit_time : 1.144596\n",
      "score_time : 0.028110\n",
      "test_precision_macro : 50%\n",
      "test_recall_macro : 50%\n",
      "test_f1 : 67%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:23:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:23:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:23:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:23:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:23:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=25).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script complete: 341.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\louis\\desktop\\files\\research project spam\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def models(train_features, train_dev_features, test_features, train_labels, train_dev_labels, test_labels, model_process\n",
    "           , feature_size, test_set):\n",
    "    # function to hold classifiers, fit and prediction and finally report the performance based on return method.\n",
    "    algorithm_names = ['k-Neighbors Classifier',\n",
    "                       'MLP Neural Network 1',\n",
    "                       'MLP Neural Network 2',\n",
    "                       'Logistic Regression',\n",
    "                       'Random Forest',\n",
    "                       'xgBoost',\n",
    "                       'Multinomial Naive Bayes',\n",
    "                       'Gaussian NB',\n",
    "                       'Bernoulli NB',\n",
    "                       'Rbf SVC',\n",
    "                       'Linear SVC',\n",
    "                       'Poly SVC',\n",
    "                       'Sigmoid SVC']\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "\n",
    "    print('Processing models...')\n",
    "\n",
    "    model1 = KNeighborsClassifier(algorithm='brute')\n",
    "    model2 = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=25)\n",
    "    model3 = MLPClassifier(hidden_layer_sizes=(150, 150), solver='lbfgs', max_iter=25)\n",
    "    model4 = LogisticRegression(solver='lbfgs', max_iter=25)\n",
    "    model5 = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model6 = XGBClassifier()\n",
    "    model7 = MultinomialNB()\n",
    "    model8 = GaussianNB()\n",
    "    model9 = BernoulliNB()\n",
    "    model10 = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=200,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model11 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model12 = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=25,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model13 = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=100,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12,\n",
    "              model13]\n",
    "\n",
    "    if model_process:\n",
    "        process_id = int(input(\"Test or development? (0/1)\"))\n",
    "        if process_id == 0:\n",
    "            print(\"x-val train_set\")\n",
    "            score0 = cross_validate(model1, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score1 = cross_validate(model2, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score2 = cross_validate(model3, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score3 = cross_validate(model4, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score4 = cross_validate(model5, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score5 = cross_validate(model6, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score6 = cross_validate(model7, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score7 = cross_validate(model8, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score8 = cross_validate(model9, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score9 = cross_validate(model10, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score10 = cross_validate(model11, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                     return_train_score=False)\n",
    "            score11 = cross_validate(model12, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                     return_train_score=False)\n",
    "            scores = [score0, score1, score2, score3, score4, score5, score6, score7, score8, score9, score10, score11]\n",
    "            print('Train set model output...\\n')\n",
    "            for i, d in enumerate(scores):\n",
    "                print(algorithm_names[i])\n",
    "                for c, e in enumerate(scoring_parse_labels):\n",
    "                    item = d.pop(e)\n",
    "                    item = item.astype(np.float)\n",
    "                    if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                        print(scoring_parse_labels[c], ':', '%0.6f' % (np.mean(item)))\n",
    "                    else:\n",
    "                        print(scoring_parse_labels[c], ':', '%0.0f' % (float(np.mean(item) * 100)) + '%')\n",
    "                        if scoring_parse_labels[c] == 'test_f1':\n",
    "                            print('\\n')\n",
    "        elif process_id == 1:\n",
    "            print(\"ROC curve development\")\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "            X, y = train_dev_features, train_dev_labels\n",
    "            k = 0\n",
    "            save_output = str(input('model record output (y/n): ')) == 'y'\n",
    "            scores = []\n",
    "            for model in models:\n",
    "                i = 0\n",
    "                if save_output:\n",
    "                    k != roc_curve(X, y, model, cv, algorithm_names[k], i)\n",
    "                else:\n",
    "                    scores.append(cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False))\n",
    "\n",
    "            for i, d in enumerate(scores):\n",
    "                print(algorithm_names[i])\n",
    "                for c, e in enumerate(scoring_parse_labels):\n",
    "                    item = d.pop(e)\n",
    "                    item = item.astype(np.float)\n",
    "                    if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                        print(scoring_parse_labels[c], ':', '%0.6f' % (np.mean(item)))\n",
    "                    else:\n",
    "                        print(scoring_parse_labels[c], ':', '%0.0f' % (float(np.mean(item) * 100)) + '%')\n",
    "                        if scoring_parse_labels[c] == 'test_f1':\n",
    "                            print('\\n')\n",
    "\n",
    "    else:\n",
    "        print(\"ROC Curve output\")\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "        X, y = test_features, test_labels\n",
    "        k = 0\n",
    "        for model in models:\n",
    "            i = 0\n",
    "            k != roc_curve(X, y, model, cv, algorithm_names[k], i)\n",
    "\n",
    "\n",
    "def roc_curve(X, y, model, cv, algorithm_name, i):\n",
    "    try:\n",
    "        # tprs = []\n",
    "        aucs = []\n",
    "        # mean_fpr = np.linspace(0, 1, 100)\n",
    "        for train, test in cv.split(X, y):\n",
    "            probas_ = model.fit(X[train], y[train]).predict_proba(X[test])\n",
    "            # Compute ROC curve and area the curve\n",
    "            fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "            # tprs.append(interp1d(mean_fpr, fpr, tpr))\n",
    "            # tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.5, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "            i += 1\n",
    "        # mean_tpr = np.mean(tprs, axis=0)\n",
    "        # mean_tpr[-1] = 1.0\n",
    "        # mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        # plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "        #          label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        #          lw=2, alpha=.8)\n",
    "        # std_tpr = np.std(tprs, axis=0)\n",
    "        # tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        # tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        # plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "        #                  label=r'$\\pm$ 1 std. dev.')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Development - ROC: ' + algorithm_name)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(\n",
    "            \"../results/plots/dev/devROC_%s_%0i_features_%0i_test.png\" % (\n",
    "                algorithm_name, feature_size, len(test_set)),\n",
    "            dpi=100,\n",
    "            facecolor='w', edgecolor='b', linewidth=1, orientation='portrait', papertype=None,\n",
    "            format=\"png\", transparent=False, bbox_inches=None, pad_inches=0.1, frameon=None)\n",
    "        print(\"Created %s ROC figure\" % (algorithm_name))\n",
    "        plt.close()\n",
    "    except (AttributeError, OverflowError) as detail:\n",
    "        print(algorithm_name + \" Failed due to \", detail)\n",
    "\n",
    "    return 1\n",
    "\n",
    "\n",
    "verbose_logs = False\n",
    "train_dev_features = []\n",
    "train_dev_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_size = int(input(\"Type amount of features to use: \"))\n",
    "    model_process = str(input(\"Extract features or Test models? [Extract features = y, Test models = n\")) == 'y'\n",
    "    verbose_logs = str(input(\"Enable verbose logs? [y/n]\")) == 'y'\n",
    "    time_initiate = time()\n",
    "    a_files = [\"../data/processed_ham.txt\", \"../data/processed_spam.txt\"]\n",
    "    a_exist = [f for f in a_files if os.path.isfile(f)]\n",
    "    usePreprocessedDatasets = False\n",
    "    if a_exist:\n",
    "        usePreprocessedDatasets = str(\n",
    "            input(\"Preprocessed datasets for ham and spam found, would you like to use them? [y/n]\")) == 'y'\n",
    "\n",
    "    if usePreprocessedDatasets:\n",
    "        ham_collection = load_(a_files[0], \"ham\")\n",
    "        spam_collection = load_(a_files[1], \"spam\")\n",
    "    else:\n",
    "        ham_collection, spam_collection = enron_selector()\n",
    "    dictionary = dictionary_build((ham_collection + spam_collection))\n",
    "    dictionary = dictionary.most_common(feature_size)\n",
    "    print(dictionary, \"\\n\")\n",
    "    train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels = calculate(ham_collection,\n",
    "                                                                                                spam_collection)\n",
    "    if model_process:\n",
    "        train_features = extract_features(train_set, \"train\")\n",
    "        train_dev_features = extract_features(train_dev_set, \"train_dev\")\n",
    "        test_features, test_labels = 0, 0\n",
    "    else:\n",
    "        test_features = extract_features(test_set, \"test\")\n",
    "        train_features, train_labels = 0, 0\n",
    "        train_dev_features, train_dev_labels = 0, 0\n",
    "    models(\n",
    "        train_features,\n",
    "        train_dev_features,\n",
    "        test_features,\n",
    "        train_labels,\n",
    "        train_dev_labels,\n",
    "        test_labels,\n",
    "        model_process,\n",
    "        feature_size,\n",
    "        test_set\n",
    "    )\n",
    "    feature_test(train_dev_features, train_dev_labels)\n",
    "    print(\"Script complete: %0.1f\" % (time() - time_initiate))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def svm_test(X, y):  # function test for SVM models specified.\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X, y)\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    max_iter = [10, 25, 50, 75, 100, 150, 200, 2000, 5000, 8000, 10000, 15000, 20000, 50000, 100000]\n",
    "    for i in range(len(max_iter)):\n",
    "        print(\"iterations: \" + str(max_iter[i]))\n",
    "        model = svm.LinearSVC(max_iter=max_iter[i])\n",
    "        scores.append(cross_validate(model, x_scaled, y, cv=cv, scoring=scoring, return_train_score=False))\n",
    "\n",
    "    with open('../results/svm_svc_test.csv', 'w', newline='') as csvfile:  # function to save output to csv file.\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "svm_test(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mpnn_test(train, test):  # Multi-layer perceptron neural network test.\n",
    "    nu_val = [10, 25, 50, 75, 100, 150, 200]\n",
    "    h_layers = [1, 2, 3, 4, 5]\n",
    "    iter_val = [10, 25, 50, 100, 200]\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    for a in range(len(nu_val)):\n",
    "        for b in range(len(h_layers)):\n",
    "            if b > 0:\n",
    "                if b == 1:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a]\n",
    "                elif b == 2:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 3:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 4:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "            else:\n",
    "                nu_layer_val = nu_val[a]\n",
    "            for c in range(len(iter_val)):\n",
    "                print('%0.0i out of %0.0i/ %0.0i' %\n",
    "                      (i, (int(len(nu_val) * len(h_layers) * len(iter_val))),\n",
    "                       int(i * (100 / (int(len(nu_val) * len(h_layers) * len(iter_val)))))) + '%',\n",
    "                      end='\\r', flush=True)\n",
    "                model = MLPClassifier(hidden_layer_sizes=(nu_layer_val), solver='lbfgs', max_iter=iter_val[c])\n",
    "                scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "                i += 1\n",
    "    with open('../results/mpnn_test.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "mpnn_test(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_2(train, test):\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(train)\n",
    "    i = 0\n",
    "    kernal_val = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    iter_val = [10, 25, 50, 100, 200]\n",
    "    for a in range(len(kernal_val)):\n",
    "        for b in range(len(iter_val)):\n",
    "            print('%0.0i out of %0.0i/ %0.0i' %\n",
    "                  (i, (int(len(kernal_val) * len(iter_val))), int(i * (100 / (int(len(kernal_val) * len(iter_val))))))\n",
    "                  + '%', end='\\r', flush=True)\n",
    "            model = svm.SVC(C=1.0, kernel=kernal_val[a], degree=3, gamma='auto', coef0=0.0, shrinking=True,\n",
    "                            probability=True, tol=0.001, cache_size=10000, class_weight=None, verbose=False,\n",
    "                            max_iter=iter_val[b], decision_function_shape='ovr', random_state=None)\n",
    "            scores.append(cross_validate(model, scaled_data, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "            i += 1\n",
    "    with open('../results/test_2.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "test_2(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_3(train, test):  # todo Knn classifier test.\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    algorithm_val = ['ball_tree', 'kd_tree', 'brute']\n",
    "    for a in range(len(algorithm_val)):\n",
    "        model = KNeighborsClassifier(algorithm=algorithm_val[a])\n",
    "        scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "        i += 1\n",
    "    with open('../results/test_3.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "test_3(train_dev_features, train_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_4(train, test):\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    model = LogisticRegression()\n",
    "    scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "    i += 1\n",
    "    with open('../results/test_4.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "test_4(train_dev_features, train_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo: implement t-test with Bonferroni correction, if not can this be done in excel with current results?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo: take best models and remove some of the pre-processing stages e.g. lemmatize, stop work and html tag removal,\n",
    "# provide scores and potentially roc curves too."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo: implement confusion matrix."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}