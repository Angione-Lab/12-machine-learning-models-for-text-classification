{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Python Version\n",
    "Check Your Python version before running this notebook.\n",
    "- Python 3.6.X is required to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use Python verison 3.6.x\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class PythonVersionException(Exception):\n",
    "    print('Please use Python verison 3.6.x')\n",
    "    pass;\n",
    "\n",
    "\n",
    "if re.match('3.6*', sys.version.split('(')[0]) is None:\n",
    "    raise PythonVersionException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Library imports\n",
    "Import all the library's required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NLTK Corpus Sets\n",
    "Run this section to check if the following corpus datasets have been downloaded, if they are missing this will download\n",
    "them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltkDataDir = '../data/nltk_data'\n",
    "\n",
    "nltk.data.path.append(os.path.abspath(nltkDataDir))\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('stopwords', nltkDataDir)\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"testing\")\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('punkt', nltkDataDir)\n",
    "    nltk.download('wordnet', nltkDataDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "\n",
    "\n",
    "def enron_selector():  # function to identify platform and selected dataset to be applied.\n",
    "    def init_lists(folder_collection, label):  # function to retrieve and apply email content to array.\n",
    "        a_list = []\n",
    "        doc_id = 0\n",
    "        file_list = []\n",
    "        label = \"Loading \" + label + \"...\"\n",
    "        for entry in folder_collection:\n",
    "            b_list = os.listdir(entry)\n",
    "            for item in b_list:\n",
    "                file_list.append(entry + item)\n",
    "        for a_file in file_list:\n",
    "            f = open(a_file, 'r')\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, file_list, label)\n",
    "            try:\n",
    "                a_list.append(f.read())\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            doc_id += 1\n",
    "        f.close()\n",
    "        return a_list\n",
    "\n",
    "    enron_ = ['Enron1/', 'Enron2/', 'Enron3/', 'Enron4/', 'Enron5/', 'Enron6/']\n",
    "    spam = []\n",
    "    ham = []\n",
    "\n",
    "    for i, sub in enumerate(enron_):\n",
    "        spam.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'spam/')\n",
    "        ham.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'ham/')\n",
    "\n",
    "    spam = init_lists(spam, \"spam\")\n",
    "    ham = init_lists(ham, \"ham\")\n",
    "    all_emails = [(email, 'spam') for email in spam]\n",
    "    all_emails += [(email, 'ham') for email in ham]\n",
    "    ham_emails, spam_emails = preprocess(all_emails)\n",
    "    ham_file, spam_file = \"../data/processed_ham.txt\", \"../data/processed_spam.txt\"\n",
    "    print(\"Writing ham file...\")\n",
    "    with open(ham_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in ham_emails))\n",
    "    print(\"Writing spam file...\")\n",
    "    with open(spam_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in spam_emails))\n",
    "    return ham_emails, spam_emails\n",
    "\n",
    "\n",
    "def test_collection(test_select):  # function that outlines all tests to be carried out.\n",
    "    if test_select == 1:\n",
    "        data_size = 1000 / 2\n",
    "    elif test_select == 2:\n",
    "        data_size = 2000 / 2\n",
    "    elif test_select == 3:\n",
    "        data_size = 3000 / 2\n",
    "    elif test_select == 4:\n",
    "        data_size = 1500 / 2\n",
    "    return data_size\n",
    "\n",
    "\n",
    "def preprocess(collection):  # function to apply pre-processing: stop words, lemmatise.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    label = 'Pre-processing emails...'\n",
    "    entry_id = 0\n",
    "    doc_id = 0\n",
    "    processed = []\n",
    "    for entry in collection:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, collection, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            emails = ''\n",
    "            if i == 0:\n",
    "                words = []\n",
    "                for word in word_tokenize(line):\n",
    "                    item = lemmatizer.lemmatize(word.lower())\n",
    "                    if not item in stoplist:\n",
    "                        if word.isalnum() == False:\n",
    "                            pass\n",
    "                        else:\n",
    "                            emails = emails + item + ','\n",
    "                processed.append(tuple((emails, entry[1])))\n",
    "                del emails\n",
    "                entry_id += 1\n",
    "        doc_id += 1\n",
    "    email_list = []\n",
    "    ham = []\n",
    "    spam = []\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'ham':\n",
    "            email_list.append(entry)\n",
    "            ham.append(entry)\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'spam':\n",
    "            email_list.append(entry)\n",
    "            spam.append(entry)\n",
    "    return ham, spam\n",
    "\n",
    "\n",
    "def dictionary_build(all_emails, remove_common_artifacts):\n",
    "    print(\"Building Dictionary...\")\n",
    "\n",
    "    def html_list():\n",
    "        html_tag_list = []\n",
    "        location = '../data/html_tag_list.txt'\n",
    "        f = open(location, 'r')\n",
    "        for i in f:\n",
    "            html_tag_list.append(i.strip())\n",
    "        f.close()\n",
    "        return html_tag_list\n",
    "\n",
    "    html_tags = [html_list()]\n",
    "    common_email_words = ['subject', 'mail', 'cc', '``', 'email', '\\n', 'www', 'com', '\\nsubject']\n",
    "    common_artifacts = ['enron']\n",
    "    processed_emails = all_emails\n",
    "    all_words = []\n",
    "    for entry in all_emails:\n",
    "        for sentence in entry:\n",
    "            if not sentence == 'ham' or sentence == 'spam':\n",
    "                words = str(sentence).split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(word)\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = list(dictionary)\n",
    "    for item in list_to_remove:\n",
    "        if len(item) <= 1:\n",
    "            del dictionary[item]\n",
    "        elif item in html_tags:\n",
    "            del dictionary[item]\n",
    "        elif str(item).isdigit():\n",
    "            del dictionary[item]\n",
    "        elif item in common_email_words:\n",
    "            del dictionary[item]\n",
    "        elif remove_common_artifacts == False and item in common_artifacts:\n",
    "            del dictionary[item]\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def extract_features(data,\n",
    "                     label):  # function to extract features to matrix based on calculating occurrence of words based\n",
    "    # on dictionary.\n",
    "    features_matrix = np.zeros((len(data), len(dictionary)))\n",
    "    label = 'Feature extraction \\'' + label + '\\':'\n",
    "    doc_id = 0\n",
    "    all_words = []\n",
    "    for entry in data:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, data, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            if i == 0:\n",
    "                # print('[' + str(doc_id) + '] ', entry)\n",
    "                words = line.split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(words)\n",
    "                    for j, d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            word_id = j\n",
    "                            features_matrix[doc_id, word_id] = words.count(word)\n",
    "        doc_id = doc_id + 1\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calculate(ham, spam):\n",
    "    main_proportion = 0.8\n",
    "    ham_size = int(len(ham) * main_proportion)\n",
    "    ham_train, ham_test = ham[:ham_size], ham[ham_size:]\n",
    "    spam_size = int(len(spam) * main_proportion)\n",
    "    spam_train, spam_test = spam[:spam_size], spam[spam_size:]\n",
    "    ham_train_size, spam_train_size = int(len(ham_train) * main_proportion), int(len(spam_train) * main_proportion)\n",
    "    ham_train, ham_train_dev = ham_train[:ham_train_size], ham_train[ham_train_size:]\n",
    "    spam_train, spam_train_dev = spam_train[:spam_train_size], spam_train[spam_train_size:]\n",
    "    train_set, train_dev_set, test_set = ham_train + spam_train, ham_train_dev + spam_train_dev, ham_test + spam_test\n",
    "    train_labels = np.zeros(len(train_set))\n",
    "    train_labels[(int((len(train_set)) - len(spam_train))):len(train_set)] = 1\n",
    "    train_dev_labels = np.zeros(len(train_dev_set))\n",
    "    train_dev_labels[(int((len(train_dev_set)) - len(spam_train_dev))):len(train_dev_set)] = 1\n",
    "    test_labels = np.zeros(len(test_set))\n",
    "    test_labels[(int((len(test_set)) - len(spam_test))):len(test_set)] = 1\n",
    "    print(\"Train set:\\n\", \"Ham: \", str(len(ham_train)), \"\\n\", \"Spam: \", str(len(spam_train)),\n",
    "          \"\\nTrain_Dev:\\n Ham:\", str(len(ham_train_dev)), \"\\n Spam:\", str(len(spam_train_dev)),\n",
    "          \"\\nTest set:\\n\", \"Ham: \", str(len(ham_test)), \"\\n\", \"Spam: \", str(len(spam_test)))\n",
    "    return train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels\n",
    "\n",
    "\n",
    "def load_(file, label):\n",
    "    if verbose_logs == False:\n",
    "        print('Loading ' + label + ' dataset')\n",
    "    with open(file, 'r') as fp:\n",
    "        values = []\n",
    "        doc_id = 0\n",
    "        size_file = fp.read().split(\";\")\n",
    "        for item in size_file:\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, size_file, \"loading \" + label + \" file...\")\n",
    "            values.append(item.split(\", \"))\n",
    "            doc_id += 1\n",
    "        return values\n",
    "\n",
    "\n",
    "def process_status(id, data, label):\n",
    "    if id + 1 < int(len(data)):\n",
    "        end_atp = \"\\r\"\n",
    "    elif id + 1 <= int(len(data)):\n",
    "        end_atp = \"\\n\"\n",
    "    return print(label, '%0.0i out of %0.0i: %0.0i' %\n",
    "                 (id + 1, len(data), int((id + 1) * (100 / len(data)))), '%', end='\\r', flush=True)\n",
    "\n",
    "\n",
    "def roc_curve_method(X, y, model, cv, algorithm_name, i):\n",
    "    try:\n",
    "        # tprs = []\n",
    "        aucs = []\n",
    "        # mean_fpr = np.linspace(0, 1, 100)\n",
    "        for train, test in cv.split(X, y):\n",
    "            probas_ = model.fit(X[train], y[train]).predict_proba(X[test])\n",
    "            # Compute ROC curve and area the curve\n",
    "            fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "            # tprs.append(interp1d(mean_fpr, fpr, tpr))\n",
    "            # tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.5, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "            i += 1\n",
    "        # mean_tpr = np.mean(tprs, axis=0)\n",
    "        # mean_tpr[-1] = 1.0\n",
    "        # mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        # plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "        #          label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        #          lw=2, alpha=.8)\n",
    "        # std_tpr = np.std(tprs, axis=0)\n",
    "        # tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        # tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        # plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "        #                  label=r'$\\pm$ 1 std. dev.')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Development - ROC: ' + algorithm_name)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(\n",
    "            \"../results/plots/dev/devROC_%s_%0i_features_%0i_test.png\" % (\n",
    "                algorithm_name, feature_size, len(test_set)),\n",
    "            dpi=100,\n",
    "            facecolor='w', edgecolor='b', linewidth=1, orientation='portrait', papertype=None,\n",
    "            format=\"png\", transparent=False, bbox_inches=None, pad_inches=0.1, frameon=None)\n",
    "        print(\"Created %s ROC figure\" % (algorithm_name))\n",
    "        plt.close()\n",
    "    except (AttributeError, OverflowError) as detail:\n",
    "        print(algorithm_name + \" Failed due to \", detail)\n",
    "\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ham dataset\n",
      "Loading spam dataset\n",
      "Building Dictionary...\n",
      "[('ect', 35672), ('company', 28725), ('please', 20344), ('ha', 20101), ('spam', 17907), ('wa', 17822), ('hou', 17264), ('would', 15531), ('new', 15268), ('time', 14848), ('price', 14224), ('business', 13582), ('may', 13139), ('information', 13117), ('one', 12342), ('gas', 11954), ('said', 11889), ('market', 11671), ('get', 11498), ('energy', 11463), ('year', 11415), ('http', 11175), ('day', 10853), ('need', 10847), ('message', 10769), ('stock', 10472), ('deal', 10058), ('know', 9782), ('pm', 9676), ('service', 9642), ('also', 9232), ('report', 9002), ('power', 8777), ('vince', 8655), ('security', 8651), ('thanks', 8432), ('week', 8372), ('like', 8289), ('statement', 7962), ('corp', 7954), ('make', 7938), ('number', 7841), ('million', 7762), ('inc', 7398), ('group', 7390), ('could', 7342), ('risk', 7182), ('sent', 7175), ('share', 7168), ('product', 7075), ('trading', 6984), ('investment', 6953), ('money', 6862), ('see', 6784), ('work', 6718), ('system', 6681), ('want', 6425), ('let', 6383), ('forward', 6375), ('call', 6352), ('order', 6331), ('contact', 6281), ('month', 6259), ('de', 6188), ('free', 6165), ('financial', 6154), ('within', 6090), ('next', 6072), ('last', 6019), ('term', 5955), ('go', 5933), ('credit', 5927), ('houston', 5868), ('take', 5862), ('name', 5850), ('offer', 5722), ('change', 5704), ('state', 5634), ('best', 5628), ('list', 5582), ('today', 5575), ('date', 5573), ('question', 5562), ('meeting', 5544), ('use', 5531), ('address', 5511), ('management', 5506), ('news', 5449), ('project', 5388), ('two', 5386), ('account', 5354), ('billion', 5315), ('right', 5262), ('customer', 5177), ('future', 5144), ('based', 5124), ('site', 5116), ('th', 5092), ('office', 5090), ('first', 5060), ('investor', 5053), ('sale', 5033), ('well', 5029), ('people', 4986), ('original', 4961), ('program', 4940), ('contract', 4934), ('transaction', 4899), ('data', 4874), ('dynegy', 4856), ('kaminski', 4809), ('software', 4770), ('help', 4766), ('many', 4740), ('click', 4732), ('per', 4686), ('net', 4679), ('made', 4655), ('back', 4617), ('issue', 4563), ('result', 4557), ('online', 4499), ('plan', 4487), ('looking', 4469), ('cost', 4467), ('look', 4463), ('way', 4410), ('send', 4407), ('hour', 4398), ('rate', 4394), ('good', 4380), ('available', 4360), ('attached', 4359), ('bank', 4321), ('line', 4224), ('interest', 4223), ('mr', 4206), ('following', 4196), ('schedule', 4136), ('regard', 4134), ('home', 4124), ('much', 4106), ('research', 4053), ('start', 4004), ('process', 3974), ('part', 3970), ('john', 3947), ('position', 3939), ('review', 3939), ('fund', 3928), ('due', 3895), ('jones', 3886), ('operation', 3881), ('high', 3865), ('forwarded', 3865), ('note', 3852), ('development', 3845), ('request', 3796), ('long', 3792), ('asset', 3762), ('california', 3740), ('phone', 3732), ('provide', 3715), ('team', 3711), ('since', 3702), ('think', 3684), ('louise', 3671), ('say', 3664), ('cash', 3645), ('thank', 3641), ('find', 3616), ('file', 3589), ('doe', 3585), ('friday', 3536), ('current', 3523), ('internet', 3516), ('value', 3510), ('buy', 3504), ('option', 3502), ('agreement', 3501), ('world', 3492), ('event', 3490), ('give', 3487), ('website', 3470), ('come', 3441), ('trade', 3414), ('dow', 3400), ('monday', 3391), ('international', 3352), ('employee', 3341), ('industry', 3333), ('act', 3326), ('total', 3319), ('going', 3316), ('dollar', 3303), ('analyst', 3299), ('receive', 3295), ('end', 3292), ('fax', 3282), ('even', 3267)] \n",
      "\n",
      "Train set:\n",
      " Ham:  10588 \n",
      " Spam:  10978 \n",
      "Train_Dev:\n",
      " Ham: 2648 \n",
      " Spam: 2745 \n",
      "Test set:\n",
      " Ham:  3310 \n",
      " Spam:  3431\n",
      "Finished feature extraction\n"
     ]
    }
   ],
   "source": [
    "verbose_logs = False\n",
    "train_dev_features = []\n",
    "train_dev_labels = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feature_size = int(input(\"Type amount of features to use: \"))\n",
    "    model_process = str(input(\"Extract features or Test models? [Extract features = y, Test models = n\")) == 'y'\n",
    "    verbose_logs = str(input(\"Enable verbose logs? [y/n]\")) == 'y'\n",
    "    exclude_common_artifacts = str(input('Would you like to include common artifacts in the dictionary? [y/n]')) == 'y'\n",
    "    a_files = [\"../data/processed_ham.txt\", \"../data/processed_spam.txt\"]\n",
    "    a_exist = [f for f in a_files if os.path.isfile(f)]\n",
    "    usePreprocessedDatasets = False\n",
    "    if a_exist:\n",
    "        usePreprocessedDatasets = str(\n",
    "            input(\"Preprocessed datasets for ham and spam found, would you like to use them? [y/n]\")) == 'y'\n",
    "\n",
    "    if usePreprocessedDatasets:\n",
    "        ham_collection = load_(a_files[0], \"ham\")\n",
    "        spam_collection = load_(a_files[1], \"spam\")\n",
    "    else:\n",
    "        ham_collection, spam_collection = enron_selector()\n",
    "    dictionary = dictionary_build((ham_collection + spam_collection), exclude_common_artifacts)\n",
    "    dictionary = dictionary.most_common(feature_size)\n",
    "    print(dictionary, \"\\n\")\n",
    "    train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels = calculate(ham_collection,\n",
    "                                                                                                spam_collection)\n",
    "    if model_process:\n",
    "        train_features = extract_features(train_set, \"train\")\n",
    "        train_dev_features = extract_features(train_dev_set, \"train_dev\")\n",
    "        test_features, test_labels = 0, 0\n",
    "    else:\n",
    "        test_features = extract_features(test_set, \"test\")\n",
    "        train_features, train_labels = 0, 0\n",
    "        train_dev_features, train_dev_labels = 0, 0\n",
    "\n",
    "    train_features_scaled = 0\n",
    "    train_dev_features_scaled = 0\n",
    "    test_features_scaled = 0\n",
    "\n",
    "    if isinstance(train_features, np.ndarray):\n",
    "        train_features_scaled = MinMaxScaler().fit_transform(train_features, train_labels)\n",
    "\n",
    "    if isinstance(train_dev_features, np.ndarray):\n",
    "        train_dev_features_scaled = MinMaxScaler().fit_transform(train_dev_features, train_dev_labels)\n",
    "\n",
    "    if isinstance(test_features, np.ndarray):\n",
    "        test_features_scaled = MinMaxScaler().fit_transform(test_features, test_labels)\n",
    "\n",
    "    print('Finished feature extraction')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def models(train_features, train_dev_features, test_features, train_labels, train_dev_labels, test_labels, model_process\n",
    "           , feature_size, test_set):\n",
    "    # function to hold classifiers, fit and prediction and finally report the performance based on return method.\n",
    "    algorithm_names = ['k-Neighbors Classifier',\n",
    "                       'MLP Neural Network 1',\n",
    "                       'MLP Neural Network 2',\n",
    "                       'Logistic Regression',\n",
    "                       'Random Forest',\n",
    "                       'xgBoost',\n",
    "                       'Multinomial Naive Bayes',\n",
    "                       'Gaussian NB',\n",
    "                       'Bernoulli NB',\n",
    "                       'Rbf SVC',\n",
    "                       'Linear SVC',\n",
    "                       'Poly SVC',\n",
    "                       'Sigmoid SVC']\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "\n",
    "    print('Processing models...')\n",
    "\n",
    "    model1 = KNeighborsClassifier(algorithm='brute')\n",
    "    model2 = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=10000)\n",
    "    model3 = MLPClassifier(hidden_layer_sizes=(150, 150), solver='lbfgs', max_iter=10000)\n",
    "    model4 = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "    model5 = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model6 = XGBClassifier()\n",
    "    model7 = MultinomialNB()\n",
    "    model8 = GaussianNB()\n",
    "    model9 = BernoulliNB()\n",
    "    model10 = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=10000,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model11 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=10000,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model12 = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=10000,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    model13 = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=10000,\n",
    "                      decision_function_shape='ovr', random_state=None)\n",
    "    models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12,\n",
    "              model13]\n",
    "    print(model_process)\n",
    "    if model_process:\n",
    "        process_type = str(input(\"Would you like to use the Test or Development sets? (y: test / n: dev)\")) == 'y'\n",
    "        print(process_type)\n",
    "        if process_type == True:\n",
    "            print(\"x-val train_set\")\n",
    "            score0 = cross_validate(model1, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score1 = cross_validate(model2, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score2 = cross_validate(model3, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score3 = cross_validate(model4, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score4 = cross_validate(model5, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score5 = cross_validate(model6, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score6 = cross_validate(model7, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score7 = cross_validate(model8, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score8 = cross_validate(model9, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score9 = cross_validate(model10, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                    return_train_score=False)\n",
    "            score10 = cross_validate(model11, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                     return_train_score=False)\n",
    "            score11 = cross_validate(model12, train_features, train_labels, scoring=scoring, cv=5,\n",
    "                                     return_train_score=False)\n",
    "            scores = [score0, score1, score2, score3, score4, score5, score6, score7, score8, score9, score10, score11]\n",
    "            print('Train set model output...\\n')\n",
    "            for i, d in enumerate(scores):\n",
    "                print(algorithm_names[i])\n",
    "                for c, e in enumerate(scoring_parse_labels):\n",
    "                    item = d.pop(e)\n",
    "                    item = item.astype(np.float)\n",
    "                    if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                        print(scoring_parse_labels[c], ':', '%0.6f' % (np.mean(item)))\n",
    "                    else:\n",
    "                        print(scoring_parse_labels[c], ':', '%0.0f' % (float(np.mean(item) * 100)) + '%')\n",
    "                        if scoring_parse_labels[c] == 'test_f1':\n",
    "                            print('\\n')\n",
    "        else:\n",
    "            print(\"ROC curve development\")\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "            k = 0\n",
    "            save_output = str(input('model record output (y/n): ')) == 'y'\n",
    "            scores = []\n",
    "            for model in models:\n",
    "                i = 0\n",
    "                if save_output:\n",
    "                    roc_curve_method(train_dev_features, train_dev_labels, model, cv, algorithm_names[k], i)\n",
    "                    k += 1\n",
    "                else:\n",
    "                    scores.append(cross_validate(\n",
    "                        model, train_dev_features, train_dev_labels, cv=cv, scoring=scoring, return_train_score=False\n",
    "                    ))\n",
    "\n",
    "            scores_df = pd.DataFrame(scores)\n",
    "\n",
    "            for i, d in enumerate(scores):\n",
    "                print(algorithm_names[i])\n",
    "                for c, e in enumerate(scoring_parse_labels):\n",
    "                    item = d.pop(e)\n",
    "                    item = item.astype(np.float)\n",
    "                    if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                        print(scoring_parse_labels[c], ':', '%0.6f' % (np.mean(item)))\n",
    "                    else:\n",
    "                        print(scoring_parse_labels[c], ':', '%0.0f' % (float(np.mean(item) * 100)) + '%')\n",
    "                        if scoring_parse_labels[c] == 'test_f1':\n",
    "                            print('\\n')\n",
    "\n",
    "    else:\n",
    "        print(\"ROC Curve output\")\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "        X, y = test_features, test_labels\n",
    "        k = 0\n",
    "        for model in models:\n",
    "            i = 0\n",
    "            k != roc_curve_method(X, y, model, cv, algorithm_names[k], i)\n",
    "\n",
    "\n",
    "models(train_features, train_dev_features, test_features, train_labels, train_dev_labels, test_labels, model_process,\n",
    "       feature_size, test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SHAP Reports\n",
    "The following section produces the SHAP reports based explaining why a model is coming to the conclusions based on the\n",
    "top 10 features.\n",
    "\n",
    "## Shap Report Method\n",
    "Below is the code to output the SHAP report providing the features as x, labels as y, dictionary used and the model to\n",
    "test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shap_report(X, y, dictionary, model, model_name):\n",
    "    formatted_dictionary = []\n",
    "    for item in dictionary:\n",
    "        formatted_dictionary.append(item[0])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "    X = pd.DataFrame(X, columns=formatted_dictionary)\n",
    "    y = pd.DataFrame(y)\n",
    "\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X, y.values.ravel(), random_state=1)\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    med = train_X.mean().values.reshape((1, train_X.shape[1]))\n",
    "\n",
    "    # Create object that can calculate shap values\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, med)\n",
    "    # Calculate Shap values\n",
    "    shap_values = explainer.shap_values(val_X.iloc[0:len(train_X), :], nsamples=len(train_X))\n",
    "\n",
    "    report_file_name = ''\n",
    "\n",
    "    shap.summary_plot(shap_values[1], val_X.iloc[0:len(train_X), :], formatted_dictionary, show=False, color_bar=True,\n",
    "                      max_display=10,\n",
    "                      plot_size=(9.6, 7.4))\n",
    "    report_file_name = str(len(dictionary)) + '_features_' + model_name + '_shap_plot_beeswarm_' +\n",
    "                       datetime.now().strftime('%Y-%m-%dT%H-%M-%S%z') + '_.svg'\n",
    "\n",
    "    plt.savefig(\n",
    "        '../results/shap/' + report_file_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Development Tests\n",
    "The following steps below were created to optimise the models and infer the reasoning around the classification.\n",
    "The results specified in the paper do not use these steps as we purely reserve the testing set for testing purpose only\n",
    "this is to avoid any potential bias with optimising models.\n",
    "\n",
    "### Multilayer Perceptron Neural Network SHAP Report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1686 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8df5fc84639143d4b6f01a4f7ac4d6d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpnn_model = MLPClassifier(hidden_layer_sizes=5, solver='lbfgs', max_iter=10000)\n",
    "shap_report(test_features, test_labels, dictionary, mpnn_model, 'mpnn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost classifier SHAP Report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:06:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1686 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fab1399df73d42c0bf3f4e5be8539991"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "shap_report(test_features, test_labels, dictionary, xgb_model, 'xgboost')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest SHAP Report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1686 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "675fb3b2eb1b46f4a6fec8ab7eb661b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfc_model = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "shap_report(test_features, test_labels, dictionary, rfc_model, 'random_forest')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Performance Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:58:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:58:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:58:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:58:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:58:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature test finished.\n"
     ]
    }
   ],
   "source": [
    "def feature_test(X, y):  # function to test and record via csv, all algorithms selected.\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['model_name', 'fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro',\n",
    "                            'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    model1 = KNeighborsClassifier(algorithm='brute')\n",
    "    model2 = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=10000)\n",
    "    model3 = MLPClassifier(hidden_layer_sizes=(150, 150), solver='lbfgs', max_iter=10000)\n",
    "    model4 = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "    model5 = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    model6 = XGBClassifier()\n",
    "    model7 = MultinomialNB()\n",
    "    model8 = GaussianNB()\n",
    "    model9 = BernoulliNB()\n",
    "    model10 = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    model11 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    model12 = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    model13 = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12,\n",
    "              model13]\n",
    "\n",
    "    print('Processing features test...')\n",
    "\n",
    "    for a in range(len(models)):\n",
    "        scores.append(cross_validate(models[a], X, y, cv=cv, scoring=scoring, return_train_score=False))\n",
    "\n",
    "    all_scores = []\n",
    "    with open('../results/feature_test_1.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            processed_scores_no_mean = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                if e == 'model_name':\n",
    "                    model_name = type(models[i]).__name__\n",
    "                    if model_name.lower() == 'svc':\n",
    "                        processed_scores.append(model_name + '_' + models[i].kernel)\n",
    "                        processed_scores_no_mean.append(model_name + '_' + models[i].kernel)\n",
    "                    else:\n",
    "                        processed_scores.append(type(models[i]).__name__)\n",
    "                        processed_scores_no_mean.append(type(models[i]).__name__)\n",
    "                else:\n",
    "                    item = d.pop(e)\n",
    "                    item = item.astype(np.float)\n",
    "                    processed_scores_no_mean.append(item)\n",
    "                    mean_value = np.mean(item)\n",
    "                    if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                        processed_scores.append('%0.6f' % mean_value)\n",
    "                    else:\n",
    "                        processed_scores.append('%0.2f' % (float(mean_value)))\n",
    "\n",
    "            filewriter.writerow(processed_scores)\n",
    "            all_scores.append(processed_scores_no_mean)\n",
    "\n",
    "    print('Feature test finished.')\n",
    "    df = pd.DataFrame(all_scores, columns=scoring_parse_labels)\n",
    "    df.to_csv('dataframe.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "# t_scores =  feature_test(train_dev_features, train_dev_labels)\n",
    "t_scores = feature_test(test_features_scaled, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear SVM Hyperparameter Optimisation Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_svm_test(X, y):  # function test for SVM models specified.\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X, y)\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    max_iter = [10, 25, 50, 75, 100, 150, 200, 2000, 5000, 8000, 10000, 15000, 20000, 50000, 100000]\n",
    "    for i in range(len(max_iter)):\n",
    "        print(\"iterations: \" + str(max_iter[i]))\n",
    "        model = svm.LinearSVC(max_iter=max_iter[i])\n",
    "        scores.append(cross_validate(model, x_scaled, y, cv=cv, scoring=scoring, return_train_score=False))\n",
    "\n",
    "    with open('../results/svm_svc_test.csv', 'w', newline='') as csvfile:  # function to save output to csv file.\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "linear_svm_test(train_dev_features, train_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multilayer Perceptron Neural Network Hyperparameter Optimisation Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mpnn_test(train, test):  # Multi-layer perceptron neural network test.\n",
    "    nu_val = [10, 25, 50, 75, 100, 150, 200]\n",
    "    h_layers = [1, 2, 3, 4, 5]\n",
    "    iter_val = [10, 25, 50, 100, 200]\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    for a in range(len(nu_val)):\n",
    "        for b in range(len(h_layers)):\n",
    "            if b > 0:\n",
    "                if b == 1:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a]\n",
    "                elif b == 2:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 3:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 4:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "            else:\n",
    "                nu_layer_val = nu_val[a]\n",
    "            for c in range(len(iter_val)):\n",
    "                print('%0.0i out of %0.0i/ %0.0i' %\n",
    "                      (i, (int(len(nu_val) * len(h_layers) * len(iter_val))),\n",
    "                       int(i * (100 / (int(len(nu_val) * len(h_layers) * len(iter_val)))))) + '%',\n",
    "                      end='\\r', flush=True)\n",
    "                model = MLPClassifier(hidden_layer_sizes=(nu_layer_val), solver='lbfgs', max_iter=iter_val[c])\n",
    "                scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "                i += 1\n",
    "    with open('../results/mpnn_test.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "mpnn_test(train_dev_features, train_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM Hyperparameter Optimisation Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def svm_tests(train, test):\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(train)\n",
    "    i = 0\n",
    "    kernal_val = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    iter_val = [10, 25, 50, 100, 200]\n",
    "    for a in range(len(kernal_val)):\n",
    "        for b in range(len(iter_val)):\n",
    "            print('%0.0i out of %0.0i/ %0.0i' %\n",
    "                  (i, (int(len(kernal_val) * len(iter_val))), int(i * (100 / (int(len(kernal_val) * len(iter_val))))))\n",
    "                  + '%', end='\\r', flush=True)\n",
    "            model = svm.SVC(C=1.0, kernel=kernal_val[a], degree=3, gamma='auto', coef0=0.0, shrinking=True,\n",
    "                            probability=True, tol=0.001, cache_size=10000, class_weight=None, verbose=False,\n",
    "                            max_iter=iter_val[b], decision_function_shape='ovr', random_state=None)\n",
    "            scores.append(cross_validate(model, scaled_data, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "            i += 1\n",
    "    with open('../results/test_2.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "svm_tests(train_dev_features, train_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-Nearest Neighbors Hyperparameter Optimisation Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def kn_test(train, test):  # todo Knn classifier test.\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    algorithm_val = ['ball_tree', 'kd_tree', 'brute']\n",
    "    for a in range(len(algorithm_val)):\n",
    "        model = KNeighborsClassifier(algorithm=algorithm_val[a])\n",
    "        scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "        i += 1\n",
    "    with open('../results/test_3.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "kn_test(train_dev_features, train_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression Hyperparameter Optimisation Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lr_test(train, test):\n",
    "    scores = []\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "    scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    i = 0\n",
    "    model = LogisticRegression()\n",
    "    scores.append(cross_validate(model, train, test, cv=cv, scoring=scoring, return_train_score=False))\n",
    "    i += 1\n",
    "    with open('../results/test_4.csv', 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(scoring_parse_labels)\n",
    "        for i, d in enumerate(scores):\n",
    "            processed_scores = []\n",
    "            for c, e in enumerate(scoring_parse_labels):\n",
    "                item = d.pop(e)\n",
    "                item = item.astype(np.float)\n",
    "                if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                    processed_scores.append('%0.6f' % (np.mean(item)))\n",
    "                else:\n",
    "                    processed_scores.append('%0.2f' % (float(np.mean(item))))\n",
    "            filewriter.writerow(processed_scores)\n",
    "\n",
    "\n",
    "lr_test(train_dev_features, train_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features test...\n",
      "batch: 1 | mode: KNeighborsClassifier. processing...\n",
      "batch: 2 | mode: KNeighborsClassifier. processing...\n",
      "batch: 3 | mode: KNeighborsClassifier. processing...\n",
      "batch: 4 | mode: KNeighborsClassifier. processing...\n",
      "batch: 1 | mode: MLPClassifier. processing...\n",
      "batch: 2 | mode: MLPClassifier. processing...\n",
      "batch: 3 | mode: MLPClassifier. processing...\n",
      "batch: 4 | mode: MLPClassifier. processing...\n",
      "batch: 1 | mode: LogisticRegression. processing...\n",
      "batch: 2 | mode: LogisticRegression. processing...\n",
      "batch: 3 | mode: LogisticRegression. processing...\n",
      "batch: 4 | mode: LogisticRegression. processing...\n",
      "batch: 1 | mode: RandomForestClassifier. processing...\n",
      "batch: 2 | mode: RandomForestClassifier. processing...\n",
      "batch: 3 | mode: RandomForestClassifier. processing...\n",
      "batch: 4 | mode: RandomForestClassifier. processing...\n",
      "batch: 1 | mode: XGBClassifier. processing...\n",
      "[16:51:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "batch: 2 | mode: XGBClassifier. processing...\n",
      "[16:51:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "batch: 3 | mode: XGBClassifier. processing...\n",
      "[16:51:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "batch: 4 | mode: XGBClassifier. processing...\n",
      "[16:51:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "batch: 1 | mode: MultinomialNB. processing...\n",
      "batch: 2 | mode: MultinomialNB. processing...\n",
      "batch: 3 | mode: MultinomialNB. processing...\n",
      "batch: 4 | mode: MultinomialNB. processing...\n",
      "batch: 1 | mode: GaussianNB. processing...\n",
      "batch: 2 | mode: GaussianNB. processing...\n",
      "batch: 3 | mode: GaussianNB. processing...\n",
      "batch: 4 | mode: GaussianNB. processing...\n",
      "batch: 1 | mode: BernoulliNB. processing...\n",
      "batch: 2 | mode: BernoulliNB. processing...\n",
      "batch: 3 | mode: BernoulliNB. processing...\n",
      "batch: 4 | mode: BernoulliNB. processing...\n",
      "batch: 1 | mode: SVC. processing...\n",
      "batch: 2 | mode: SVC. processing...\n",
      "batch: 3 | mode: SVC. processing...\n",
      "batch: 4 | mode: SVC. processing...\n",
      "batch: 1 | mode: SVC. processing...\n",
      "batch: 2 | mode: SVC. processing...\n",
      "batch: 3 | mode: SVC. processing...\n",
      "batch: 4 | mode: SVC. processing...\n",
      "batch: 1 | mode: SVC. processing...\n",
      "batch: 2 | mode: SVC. processing...\n",
      "batch: 3 | mode: SVC. processing...\n",
      "batch: 4 | mode: SVC. processing...\n",
      "batch: 1 | mode: SVC. processing...\n",
      "batch: 2 | mode: SVC. processing...\n",
      "batch: 3 | mode: SVC. processing...\n",
      "batch: 4 | mode: SVC. processing...\n"
     ]
    }
   ],
   "source": [
    "def t_test_feature_generation(X, y):  # function to test and record via csv, all algorithms selected.\n",
    "    scores = []\n",
    "    iterations = 4\n",
    "\n",
    "    kn_classifier = KNeighborsClassifier(algorithm='brute')\n",
    "    mpnn_classifier = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=10000)\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    xgboost = XGBClassifier()\n",
    "    mnb = MultinomialNB()\n",
    "    gnb = GaussianNB()\n",
    "    bnb = BernoulliNB()\n",
    "    rbf_svc = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    linear_svc = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    poly_svc = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    sig_svc = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                      tol=0.001, cache_size=1000, max_iter=10000, decision_function_shape='ovr')\n",
    "    models = [kn_classifier,\n",
    "              mpnn_classifier,\n",
    "              lr,\n",
    "              rf_classifier,\n",
    "              xgboost,\n",
    "              mnb,\n",
    "              gnb,\n",
    "              bnb,\n",
    "              rbf_svc,\n",
    "              linear_svc,\n",
    "              poly_svc,\n",
    "              sig_svc\n",
    "              ]\n",
    "\n",
    "    cv_collection = []\n",
    "\n",
    "    # Ensure all cv collections are the same for each iteration of each model since shuffle is set to true.\n",
    "    for i in range(iterations):\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        cv_collection.append(cv)\n",
    "\n",
    "    print('Processing features test...')\n",
    "\n",
    "    for a in range(len(models)):\n",
    "        model = models[a]\n",
    "        model_name = type(model).__name__\n",
    "        if model_name.lower() == 'svc':\n",
    "            model_name_field = model_name + '_' + model.kernel\n",
    "        else:\n",
    "            model_name_field = type(model).__name__\n",
    "\n",
    "        model_score = []\n",
    "        for count in range(iterations):\n",
    "            print('batch: {} | model: {}, processing...'.format(count + 1, model_name_field))\n",
    "            score = cross_validate(models[a], X, y, cv=cv_collection[count], scoring='f1', return_train_score=False)\n",
    "            if len(model_score) != 0:\n",
    "                model_score = np.append(model_score,  score['test_score'])\n",
    "            else:\n",
    "                model_score.append(score['test_score'])\n",
    "\n",
    "        scores.append([model_name_field, ','.join(str(v) for v in model_score)])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=['model_name', 'f1_score'])\n",
    "    df.to_csv('t_test_values.csv')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# t_scores =  feature_test(train_dev_features, train_dev_labels)\n",
    "t_test_scores = t_test_feature_generation(test_features_scaled, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}