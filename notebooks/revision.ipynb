{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Library imports\n",
    "Import all the library's required for this notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import os.path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from IPython.core.display import display\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Python Version\n",
    "Check Your Python version before running this notebook.\n",
    "- Python 3.6.X is required to run this notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use Python verison 3.6.x\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class PythonVersionException(Exception):\n",
    "    print('Please use Python verison 3.6.x')\n",
    "    pass;\n",
    "\n",
    "\n",
    "if re.match('3.6*', sys.version.split('(')[0]) is None:\n",
    "    raise PythonVersionException\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK Corpus Sets\n",
    "Run this section to check if the following corpus datasets have been downloaded, if they are missing this will download\n",
    "them for you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltkDataDir = '../data/nltk_data'\n",
    "\n",
    "nltk.data.path.append(os.path.abspath(nltkDataDir))\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('stopwords', nltkDataDir)\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"testing\")\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('punkt', nltkDataDir)\n",
    "    nltk.download('wordnet', nltkDataDir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "remove_common_artifacts = False\n",
    "verbose_logs = False\n",
    "\n",
    "dictionary = []\n",
    "formatted_dictionary = []\n",
    "selected_set = None\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "train_dev_features = []\n",
    "train_dev_labels = []\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "all_features_scaled = 0\n",
    "train_features_scaled = 0\n",
    "train_dev_features_scaled = 0\n",
    "test_features_scaled = 0\n",
    "\n",
    "max_iterations = 10000\n",
    "\n",
    "kn_classifier = KNeighborsClassifier(algorithm='brute')\n",
    "mpnn_classifier = MLPClassifier(hidden_layer_sizes=75, solver='lbfgs', max_iter=max_iterations)\n",
    "lr_classifier = LogisticRegression(solver='lbfgs', max_iter=max_iterations)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "xgboost = XGBClassifier()\n",
    "mnb_classifier = MultinomialNB()\n",
    "gnb_classifier = GaussianNB()\n",
    "bnb_classifier = BernoulliNB()\n",
    "rbf_svc = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                  tol=0.001, cache_size=1000, max_iter=max_iterations, decision_function_shape='ovr')\n",
    "linear_svc = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                     tol=0.001, cache_size=1000, max_iter=max_iterations, decision_function_shape='ovr')\n",
    "poly_svc = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                   tol=0.001, cache_size=1000, max_iter=max_iterations, decision_function_shape='ovr')\n",
    "sig_svc = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                  tol=0.001, cache_size=1000, max_iter=max_iterations, decision_function_shape='ovr')\n",
    "\n",
    "models = [kn_classifier,\n",
    "          mpnn_classifier,\n",
    "          lr_classifier,\n",
    "          rf_classifier,\n",
    "          xgboost,\n",
    "          mnb_classifier,\n",
    "          gnb_classifier,\n",
    "          bnb_classifier,\n",
    "          rbf_svc,\n",
    "          linear_svc,\n",
    "          poly_svc,\n",
    "          sig_svc\n",
    "          ]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "scores = []\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "\n",
    "\n",
    "def enron_selector():  # function to identify platform and selected dataset to be applied.\n",
    "    print('Loading enron corpus')\n",
    "\n",
    "    def init_lists(folder_collection, label):  # function to retrieve and apply email content to array.\n",
    "        a_list = []\n",
    "        doc_id = 0\n",
    "        file_list = []\n",
    "        label = \"Loading \" + label + \"...\"\n",
    "        for entry in folder_collection:\n",
    "            b_list = os.listdir(entry)\n",
    "            for item in b_list:\n",
    "                file_list.append(entry + item)\n",
    "        for a_file in file_list:\n",
    "            f = open(a_file, 'r')\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, file_list, label)\n",
    "            try:\n",
    "                a_list.append(f.read())\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            doc_id += 1\n",
    "        f.close()\n",
    "        return a_list\n",
    "\n",
    "    enron_ = ['Enron1/', 'Enron2/', 'Enron3/', 'Enron4/', 'Enron5/', 'Enron6/']\n",
    "    spam = []\n",
    "    ham = []\n",
    "\n",
    "    for i, sub in enumerate(enron_):\n",
    "        spam.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'spam/')\n",
    "        ham.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'ham/')\n",
    "\n",
    "    spam = init_lists(spam, \"spam\")\n",
    "    ham = init_lists(ham, \"ham\")\n",
    "    all_emails = [(email, 'spam') for email in spam]\n",
    "    all_emails += [(email, 'ham') for email in ham]\n",
    "    ham_emails, spam_emails = preprocess(all_emails)\n",
    "    if remove_common_artifacts:\n",
    "        ham_file, spam_file = \"../data/processed_ham_remove_common_artifacts.txt\", \"../data/processed_spam_remove_common_artifacts.txt\"\n",
    "    else:\n",
    "        ham_file, spam_file = \"../data/processed_ham.txt\", \"../data/processed_spam.txt\"\n",
    "    print(\"Writing ham file...\")\n",
    "    with open(ham_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in ham_emails))\n",
    "    print(\"Writing spam file...\")\n",
    "    with open(spam_file, 'w') as fp:\n",
    "        fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in spam_emails))\n",
    "    return ham_emails, spam_emails\n",
    "\n",
    "\n",
    "def test_collection(test_select):  # function that outlines all tests to be carried out.\n",
    "    if test_select == 1:\n",
    "        data_size = 1000 / 2\n",
    "    elif test_select == 2:\n",
    "        data_size = 2000 / 2\n",
    "    elif test_select == 3:\n",
    "        data_size = 3000 / 2\n",
    "    elif test_select == 4:\n",
    "        data_size = 1500 / 2\n",
    "    return data_size\n",
    "\n",
    "\n",
    "def html_list():\n",
    "    html_tag_list = []\n",
    "    location = '../data/html_tag_list.txt'\n",
    "    f = open(location, 'r')\n",
    "    for i in f:\n",
    "        html_tag_list.append(i.strip())\n",
    "    f.close()\n",
    "    return html_tag_list\n",
    "\n",
    "\n",
    "def preprocess(collection):  # function to apply pre-processing: stop words, lemmatise.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print('Pre-processing emails...')\n",
    "    entry_id = 0\n",
    "    doc_id = 0\n",
    "    processed = []\n",
    "    html_tags = [html_list()]\n",
    "    common_artifacts = ['enron', 'subject', 'mail', 'cc', '``', 'email', '\\n', 'www', 'com', '\\nsubject']\n",
    "    for entry in collection:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, collection, 'Pre-processing emails...')\n",
    "        for i, line in enumerate(entry):\n",
    "            emails = ''\n",
    "            if i == 0:\n",
    "                for word in word_tokenize(line):\n",
    "                    item = lemmatizer.lemmatize(word.lower())\n",
    "                    if not item in stoplist:\n",
    "                        if word.isalnum() == False or word in html_tags:\n",
    "                            pass\n",
    "                        elif remove_common_artifacts == True and item in common_artifacts:\n",
    "                            pass\n",
    "                        else:\n",
    "                            emails = emails + item + ','\n",
    "                processed.append(tuple((emails, entry[1])))\n",
    "                del emails\n",
    "                entry_id += 1\n",
    "        doc_id += 1\n",
    "    email_list = []\n",
    "    ham = []\n",
    "    spam = []\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'ham':\n",
    "            email_list.append(entry)\n",
    "            ham.append(entry)\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'spam':\n",
    "            email_list.append(entry)\n",
    "            spam.append(entry)\n",
    "    return ham, spam\n",
    "\n",
    "\n",
    "def dictionary_build(all_emails):\n",
    "    print(\"Building Dictionary...\")\n",
    "\n",
    "    html_tags = [html_list()]\n",
    "    common_artifacts = ['enron', 'subject', 'mail', 'cc', '``', 'email', '\\n', 'www', 'com', '\\nsubject']\n",
    "    all_words = []\n",
    "    for entry in all_emails:\n",
    "        for sentence in entry:\n",
    "            if not sentence == 'ham' or sentence == 'spam':\n",
    "                words = str(sentence).split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(word)\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = list(dictionary)\n",
    "    for item in list_to_remove:\n",
    "        if len(item) <= 1:\n",
    "            del dictionary[item]\n",
    "        elif item in html_tags:\n",
    "            del dictionary[item]\n",
    "        elif str(item).isdigit():\n",
    "            del dictionary[item]\n",
    "        elif remove_common_artifacts == True and item in common_artifacts:\n",
    "            del dictionary[item]\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def extract_features(data,\n",
    "                     label):  # function to extract features to matrix based on calculating occurrence of words based\n",
    "    # on dictionary.\n",
    "    features_matrix = np.zeros((len(data), len(dictionary)))\n",
    "    print('Feature extraction \\'' + label + '\\':')\n",
    "    doc_id = 0\n",
    "    all_words = []\n",
    "    for entry in data:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, data, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            if i == 0:\n",
    "                # print('[' + str(doc_id) + '] ', entry)\n",
    "                words = line.split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(words)\n",
    "                    for j, d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            word_id = j\n",
    "                            features_matrix[doc_id, word_id] = words.count(word)\n",
    "        doc_id = doc_id + 1\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calculate(ham, spam):\n",
    "    main_proportion = 0.8\n",
    "    ham_size = int(len(ham) * main_proportion)\n",
    "    ham_train, ham_test = ham[:ham_size], ham[ham_size:]\n",
    "    spam_size = int(len(spam) * main_proportion)\n",
    "    spam_train, spam_test = spam[:spam_size], spam[spam_size:]\n",
    "    ham_train_size, spam_train_size = int(len(ham_train) * main_proportion), int(len(spam_train) * main_proportion)\n",
    "    ham_train, ham_train_dev = ham_train[:ham_train_size], ham_train[ham_train_size:]\n",
    "    spam_train, spam_train_dev = spam_train[:spam_train_size], spam_train[spam_train_size:]\n",
    "    train_set, train_dev_set, test_set = ham_train + spam_train, ham_train_dev + spam_train_dev, ham_test + spam_test\n",
    "    train_labels = np.zeros(len(train_set))\n",
    "    train_labels[(int((len(train_set)) - len(spam_train))):len(train_set)] = 1\n",
    "    train_dev_labels = np.zeros(len(train_dev_set))\n",
    "    train_dev_labels[(int((len(train_dev_set)) - len(spam_train_dev))):len(train_dev_set)] = 1\n",
    "    test_labels = np.zeros(len(test_set))\n",
    "    test_labels[(int((len(test_set)) - len(spam_test))):len(test_set)] = 1\n",
    "\n",
    "    all_set = ham_train + ham_train_dev + ham_test + spam_train + spam_train_dev + spam_test\n",
    "    all_labels = np.zeros(len(all_set))\n",
    "    all_labels[(int((len(all_labels)) - len(spam_train + spam_train_dev + spam_test))): len(all_set)] = 1\n",
    "\n",
    "    print(\"Train set:\\n\", \"Ham: \", str(len(ham_train)), \"\\n\", \"Spam: \", str(len(spam_train)),\n",
    "          \"\\nTrain_Dev:\\n Ham:\", str(len(ham_train_dev)), \"\\n Spam:\", str(len(spam_train_dev)),\n",
    "          \"\\nTest set:\\n\", \"Ham: \", str(len(ham_test)), \"\\n\", \"Spam: \", str(len(spam_test)),\n",
    "          \"\\nAll set:\\n\", \"Ham: \", str(len(ham_train + ham_train_dev + ham_test)), \"\\n\", \"Spam: \",\n",
    "          str(len(spam_train + spam_train_dev + spam_test)))\n",
    "    return train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels, all_set, all_labels\n",
    "\n",
    "\n",
    "def load_(file, label):\n",
    "    if verbose_logs == False:\n",
    "        print('Loading ' + label + ' dataset')\n",
    "    with open(file, 'r') as fp:\n",
    "        values = []\n",
    "        doc_id = 0\n",
    "        size_file = fp.read().split(\";\")\n",
    "        for item in size_file:\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, size_file, \"loading \" + label + \" file...\")\n",
    "            values.append(item.split(\", \"))\n",
    "            doc_id += 1\n",
    "        return values\n",
    "\n",
    "\n",
    "def process_status(id, data, label):\n",
    "    if id + 1 < int(len(data)):\n",
    "        end_atp = \"\\r\"\n",
    "    elif id + 1 <= int(len(data)):\n",
    "        end_atp = \"\\n\"\n",
    "    return print(label, '%0.0i out of %0.0i: %0.0i' %\n",
    "                 (id + 1, len(data), int((id + 1) * (100 / len(data)))), '%', end='\\r', flush=True)\n",
    "\n",
    "\n",
    "def determine_model_name(model):\n",
    "    model_name = type(model).__name__\n",
    "    if model_name.lower() == 'svc':\n",
    "        return model_name + '_' + model.kernel\n",
    "    else:\n",
    "        return type(model).__name__\n",
    "\n",
    "\n",
    "def handle_scores_to_csv(file_name):\n",
    "    labels = scoring_parse_labels\n",
    "    if labels[0] != 'model_name':\n",
    "        labels.insert(0, 'model_name')\n",
    "    processed_scores = []\n",
    "    for i, d in enumerate(scores):\n",
    "        processed_score = [d[0]]\n",
    "        print(d)\n",
    "        for c, e in enumerate(scoring_parse_labels):\n",
    "            item = d[1][e]\n",
    "            item = item.astype(np.float)\n",
    "            if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                processed_score.append('%0.6f' % (np.mean(item)))\n",
    "            else:\n",
    "                processed_score.append('%0.2f' % (float(np.mean(item))))\n",
    "        processed_scores.append(processed_score)\n",
    "\n",
    "    df = pd.DataFrame(processed_scores, columns=labels)\n",
    "    df.to_csv(file_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def roc_curve_report(X, y):\n",
    "    for a in range(len(models)):\n",
    "        model = model[a]\n",
    "        model_name = determine_model_name(model)\n",
    "        try:\n",
    "            aucs = []\n",
    "            for i, train, test in cv.split(X, y):\n",
    "                probas_ = model.fit(X[train], y[train]).predict_proba(X[test])\n",
    "                fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                aucs.append(roc_auc)\n",
    "                plt.plot(fpr, tpr, lw=1, alpha=0.5, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "            plt.xlim([-0.05, 1.05])\n",
    "            plt.ylim([-0.05, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC: ' + model_name)\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(\n",
    "                \"../results/plots/dev/devROC_%s_%0i_features_%0i_test.png\" % (\n",
    "                    model_name, feature_size, len(X)),\n",
    "                dpi=100,\n",
    "                facecolor='w', edgecolor='b', linewidth=1, orientation='portrait', papertype=None,\n",
    "                format=\"png\", transparent=False, bbox_inches=None, pad_inches=0.1, frameon=None)\n",
    "            print(\"Created %s ROC figure\" % model_name)\n",
    "            plt.close()\n",
    "        except (AttributeError, OverflowError) as detail:\n",
    "            print(model_name + \" Failed due to \", detail)\n",
    "\n",
    "\n",
    "def shap_report(X, y, model):\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y)\n",
    "\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X, y.values.ravel(), random_state=1)\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    med = train_X.mean().values.reshape((1, train_X.shape[1]))\n",
    "\n",
    "    # Create object that can calculate shap values\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, med)\n",
    "    # Calculate Shap values\n",
    "    shap_values = explainer.shap_values(val_X.iloc[0:len(train_X), :], nsamples=len(train_X))\n",
    "\n",
    "    shap.summary_plot(shap_values[1], val_X.iloc[0:len(train_X), :], formatted_dictionary, show=False, color_bar=True,\n",
    "                      max_display=10,\n",
    "                      plot_size=(9.6, 7.4))\n",
    "    report_file_name = str(\n",
    "        len(dictionary)) + '_features_' + determine_model_name(\n",
    "        model) + '_shap_plot_beeswarm_' + datetime.now().strftime(\n",
    "        '%Y-%m-%dT%H-%M-%S%z') + '_.svg'\n",
    "\n",
    "    plt.savefig('../results/shap/' + report_file_name)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def models_report(X, y):  # function to test and record via csv, all algorithms selected.\n",
    "    print('Processing model tests...')\n",
    "    for a in range(len(models)):\n",
    "        model = models[a]\n",
    "        model_name = determine_model_name(model)\n",
    "        print('{} testing model...'.format(model_name))\n",
    "        scores.append([model_name, cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False)])\n",
    "\n",
    "    df = handle_scores_to_csv('../results/dataframe.csv')\n",
    "    print('Models test finished.')\n",
    "    return df\n",
    "\n",
    "\n",
    "def t_test_results(X, y, iterations):  # function to test and record via csv, all algorithms selected.\n",
    "    cv_collection = []\n",
    "\n",
    "    # Ensure all cv collections are the same for each iteration of each model since shuffle is set to true.\n",
    "    for i in range(iterations):\n",
    "        cv_t = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        cv_collection.append(cv_t)\n",
    "\n",
    "    print('Processing features test...')\n",
    "\n",
    "    for a in range(len(models)):\n",
    "        model = models[a]\n",
    "        model_name = type(model).__name__\n",
    "        if model_name.lower() == 'svc':\n",
    "            model_name_field = model_name + '_' + model.kernel\n",
    "        else:\n",
    "            model_name_field = type(model).__name__\n",
    "\n",
    "        model_score = []\n",
    "        for count in range(iterations):\n",
    "            print('batch: {} | model: {}, processing...'.format(count + 1, model_name_field))\n",
    "            score = cross_validate(models[a], X, y, cv=cv_collection[count], scoring='f1', return_train_score=False)\n",
    "            if len(model_score) != 0:\n",
    "                model_score = np.append(model_score, score['test_score'])\n",
    "            else:\n",
    "                model_score.append(score['test_score'])\n",
    "\n",
    "        scores.append([model_name_field, ','.join(str(v) for v in model_score)])\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=['model_name', 'f1_score'])\n",
    "    df.to_csv('../results/scores/t_test_values.csv')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def lr_test(X, y):\n",
    "    scores.append(\n",
    "        [determine_model_name(lr_classifier),\n",
    "         cross_validate(lr_classifier, X, y, cv=cv, scoring=scoring, return_train_score=False)])\n",
    "    handle_scores_to_csv('../results/scores/logistic_regression_test.csv')\n",
    "\n",
    "\n",
    "def kn_test(X, y):\n",
    "    algorithm_val = ['ball_tree', 'kd_tree', 'brute']\n",
    "    for a in range(len(algorithm_val)):\n",
    "        model = KNeighborsClassifier(algorithm=algorithm_val[a])\n",
    "        scores.append([determine_model_name(model),\n",
    "                       cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False)])\n",
    "    handle_scores_to_csv('../results/scores/k_neighbour_classifier_test_score.csv')\n",
    "\n",
    "\n",
    "def svm_tests(X, y):\n",
    "    kernel_val = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    for a in range(len(kernel_val)):\n",
    "        if verbose_logs:\n",
    "            print('Processing {} model...'.format(kernel_val[a]))\n",
    "        model = svm.SVC(C=1.0, kernel=kernel_val[a], degree=3, gamma='auto', coef0=0.0, shrinking=True,\n",
    "                        probability=True, tol=0.001, cache_size=10000, class_weight=None, verbose=False,\n",
    "                        max_iter=max_iterations, decision_function_shape='ovr', random_state=None)\n",
    "        scores.append([determine_model_name(model),\n",
    "                       cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False)])\n",
    "    handle_scores_to_csv('../results/scores/svm_test_scores.csv')\n",
    "\n",
    "\n",
    "def mpnn_test(X, y):  # Multi-layer perceptron neural network test.\n",
    "    nu_val = [10, 25, 50, 75, 100, 150, 200]\n",
    "    h_layers = [1, 2, 3, 4, 5]\n",
    "    iter_val = [10, 25, 50, 100, 200]\n",
    "    for a in range(len(nu_val)):\n",
    "        for b in range(len(h_layers)):\n",
    "            if b > 0:\n",
    "                if b == 1:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a]\n",
    "                elif b == 2:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 3:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "                elif b == 4:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a], nu_val[a], nu_val[a]\n",
    "            else:\n",
    "                nu_layer_val = nu_val[a]\n",
    "            for c in range(len(iter_val)):\n",
    "                print('%0.0i out of %0.0i/ %0.0i' %\n",
    "                      (i, (int(len(nu_val) * len(h_layers) * len(iter_val))),\n",
    "                       int(i * (100 / (int(len(nu_val) * len(h_layers) * len(iter_val)))))) + '%',\n",
    "                      end='\\r', flush=True)\n",
    "                model = MLPClassifier(hidden_layer_sizes=(nu_layer_val), solver='lbfgs', max_iter=iter_val[c])\n",
    "                scores.append(['{}_{}'.format(determine_model_name(model), nu_layer_val),\n",
    "                               cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False)])\n",
    "    handle_scores_to_csv('../results/scores/mpnn_hp_test_scores.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# User Input\n",
    "Please select the following options to for the notebook to process, please run the cell below to render the widgets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "GridspecLayout(children=(Label(value='Preprocessing and Feature Extraction Stage:', layout=Layout(grid_area='w…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "814405c4284b49aab34b6d4c4b954014"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets, GridspecLayout\n",
    "\n",
    "defined_widgets = []\n",
    "\n",
    "preprocess_label = widgets.Label('Preprocessing and Feature Extraction Stage:')\n",
    "defined_widgets.append(preprocess_label)\n",
    "\n",
    "feature_size_widget = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=50,\n",
    "    max=1000,\n",
    "    description='Feature size'\n",
    ")\n",
    "defined_widgets.append(feature_size_widget)\n",
    "\n",
    "show_logs = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show logs during process',\n",
    ")\n",
    "defined_widgets.append(show_logs)\n",
    "\n",
    "enable_preprocess_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enable preprocessing data.',\n",
    ")\n",
    "defined_widgets.append(enable_preprocess_widget)\n",
    "\n",
    "exclude_common_artifacts = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Exclude common artifacts',\n",
    ")\n",
    "defined_widgets.append(exclude_common_artifacts)\n",
    "\n",
    "reprocess_saved_feature_set = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Use processed feature dataset',\n",
    ")\n",
    "defined_widgets.append(reprocess_saved_feature_set)\n",
    "\n",
    "test_label_widget = widgets.Label('Test Selection:')\n",
    "defined_widgets.append(test_label_widget)\n",
    "\n",
    "test_with_all = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Run all test with all data',\n",
    ")\n",
    "defined_widgets.append(test_with_all)\n",
    "\n",
    "test_with_train = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Run all test with train set',\n",
    ")\n",
    "defined_widgets.append(test_with_train)\n",
    "\n",
    "test_with_dev = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Run all test with dev set',\n",
    ")\n",
    "defined_widgets.append(test_with_dev)\n",
    "\n",
    "test_with_test = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Run all test with test set',\n",
    ")\n",
    "defined_widgets.append(test_with_test)\n",
    "\n",
    "grid = GridspecLayout(len(defined_widgets), 1)\n",
    "\n",
    "for i in range(len(defined_widgets)):\n",
    "    grid[i, 0] = defined_widgets[i]\n",
    "\n",
    "display(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ham dataset\n",
      "Loading spam dataset\n",
      "Building Dictionary...\n",
      "[('ect', 35666), ('company', 28711), ('please', 20234), ('ha', 20085), ('spam', 17847), ('wa', 17802), ('hou', 17264), ('would', 15499), ('time', 14803), ('new', 14776), ('price', 14215), ('business', 13526), ('may', 13110), ('information', 13076), ('one', 12298), ('gas', 11919), ('said', 11887), ('market', 11652), ('energy', 11411), ('year', 11365), ('get', 11176), ('http', 11136), ('day', 10835), ('message', 10689), ('need', 10656), ('stock', 10416), ('deal', 9996), ('know', 9682), ('pm', 9676), ('service', 9634), ('also', 9229), ('report', 8988), ('power', 8709), ('security', 8623), ('vince', 8621), ('thanks', 8389), ('week', 8363), ('like', 8254), ('statement', 7960), ('corp', 7950), ('make', 7856), ('number', 7838), ('million', 7755), ('inc', 7395), ('group', 7387), ('could', 7330), ('sent', 7173), ('share', 7157), ('risk', 7129), ('product', 7073), ('trading', 6956), ('investment', 6926), ('money', 6839), ('see', 6759), ('work', 6668), ('system', 6667), ('forward', 6369), ('let', 6339), ('call', 6322), ('want', 6262), ('month', 6258), ('order', 6251), ('contact', 6246), ('de', 6181), ('financial', 6123), ('within', 6090), ('free', 6058), ('next', 6026), ('last', 5968), ('term', 5935), ('go', 5906), ('credit', 5871), ('name', 5839), ('houston', 5828), ('take', 5812), ('offer', 5706), ('change', 5670), ('state', 5631), ('list', 5565), ('date', 5563), ('question', 5546), ('today', 5538), ('use', 5511), ('address', 5502), ('management', 5477), ('meeting', 5456), ('best', 5452), ('news', 5422), ('two', 5380), ('project', 5340), ('billion', 5315), ('account', 5275), ('right', 5255), ('customer', 5154), ('future', 5139), ('based', 5123), ('th', 5092), ('site', 5083), ('office', 5036), ('investor', 5033), ('first', 5009), ('sale', 5005), ('well', 5004), ('people', 4972), ('original', 4958), ('program', 4927), ('transaction', 4895), ('contract', 4887), ('data', 4858), ('dynegy', 4852), ('kaminski', 4809), ('many', 4727), ('click', 4723), ('help', 4717), ('per', 4682), ('software', 4676), ('net', 4665), ('made', 4653), ('back', 4608), ('issue', 4556), ('result', 4548), ('plan', 4481), ('cost', 4452), ('look', 4420), ('way', 4403), ('send', 4397), ('hour', 4396), ('online', 4386), ('rate', 4379), ('looking', 4373), ('available', 4355), ('attached', 4354), ('bank', 4306), ('good', 4276), ('line', 4215), ('interest', 4215), ('following', 4190), ('mr', 4144), ('regard', 4132), ('much', 4100), ('home', 4093), ('research', 4004), ('process', 3974), ('part', 3958), ('john', 3932), ('position', 3925), ('review', 3925), ('fund', 3923), ('schedule', 3921), ('due', 3892), ('jones', 3883), ('operation', 3878), ('forwarded', 3858), ('development', 3836), ('note', 3832), ('high', 3827), ('long', 3784), ('asset', 3747), ('phone', 3713), ('provide', 3706), ('since', 3700), ('team', 3700), ('think', 3663), ('say', 3655), ('request', 3643), ('louise', 3643), ('cash', 3630), ('thank', 3599), ('california', 3588), ('find', 3580), ('file', 3580), ('doe', 3556), ('friday', 3523), ('current', 3512), ('value', 3506), ('internet', 3500), ('agreement', 3496), ('option', 3490), ('event', 3488), ('world', 3483), ('website', 3465), ('give', 3457), ('come', 3430), ('dow', 3397), ('buy', 3395), ('monday', 3381), ('trade', 3380), ('start', 3378), ('international', 3347), ('employee', 3330), ('industry', 3326), ('act', 3323), ('total', 3310), ('going', 3307), ('dollar', 3303), ('end', 3285), ('receive', 3285), ('fax', 3277), ('analyst', 3277), ('even', 3265)]\n",
      "Train set:\n",
      " Ham:  10588 \n",
      " Spam:  10978 \n",
      "Train_Dev:\n",
      " Ham: 2648 \n",
      " Spam: 2745 \n",
      "Test set:\n",
      " Ham:  3310 \n",
      " Spam:  3431 \n",
      "All set:\n",
      " Ham:  16546 \n",
      " Spam:  17154\n",
      "Feature extraction 'all_set':\n",
      "Finished feature extraction\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_size = feature_size_widget.value\n",
    "    verbose_logs = show_logs.value\n",
    "    remove_common_artifacts = exclude_common_artifacts.value\n",
    "    if remove_common_artifacts:\n",
    "        a_files = [\"../data/processed_ham_remove_common_artifacts.txt\",\n",
    "                   \"../data/processed_spam_remove_common_artifacts.txt\"]\n",
    "    else:\n",
    "        a_files = [\"../data/processed_ham.txt\", \"../data/processed_spam.txt\"]\n",
    "\n",
    "    if reprocess_saved_feature_set.value:\n",
    "        ham_collection = load_(a_files[0], \"ham\")\n",
    "        spam_collection = load_(a_files[1], \"spam\")\n",
    "    else:\n",
    "        ham_collection, spam_collection = enron_selector()\n",
    "    dictionary = dictionary_build((ham_collection + spam_collection))\n",
    "    dictionary = dictionary.most_common(feature_size)\n",
    "    formatted_dictionary = []\n",
    "    for item in dictionary:\n",
    "        formatted_dictionary.append(item[0])\n",
    "\n",
    "    print(dictionary)\n",
    "    train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels, all_set, all_labels = calculate(\n",
    "        ham_collection, spam_collection)\n",
    "\n",
    "    if test_with_all.value:\n",
    "        all_features = extract_features(all_set, 'all_set')\n",
    "    elif test_with_train.value:\n",
    "        train_features = extract_features(train_set, \"train\")\n",
    "        selected_set = 'train'\n",
    "    elif test_with_dev.value:\n",
    "        train_dev_features = extract_features(train_dev_set, \"train_dev\")\n",
    "        selected_set = 'dev'\n",
    "    elif test_with_test.value:\n",
    "        test_features = extract_features(test_set, \"test\")\n",
    "        selected_set = 'test'\n",
    "\n",
    "    if isinstance(all_features, np.ndarray):\n",
    "        all_features_scaled = MinMaxScaler().fit_transform(all_features, all_labels)\n",
    "\n",
    "    if isinstance(train_features, np.ndarray):\n",
    "        train_features_scaled = MinMaxScaler().fit_transform(train_features, train_labels)\n",
    "\n",
    "    if isinstance(train_dev_features, np.ndarray):\n",
    "        train_dev_features_scaled = MinMaxScaler().fit_transform(train_dev_features, train_dev_labels)\n",
    "\n",
    "    if isinstance(test_features, np.ndarray):\n",
    "        test_features_scaled = MinMaxScaler().fit_transform(test_features, test_labels)\n",
    "\n",
    "    print('Finished feature extraction')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Selection\n",
    "Please make sure the last cell have completed the run successfully.\n",
    "\n",
    "Run this cell to determine which feature set you've selected for reporting, refer to the user input cell on which set\n",
    "was selected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset selected, size: 33700\n",
      "WARNING - ANY REPORT WILL TAKE A LONG TIME TO COMPLETE DUE TO THE SIZE OF THE DATASET SELECTED\n"
     ]
    }
   ],
   "source": [
    "if test_with_train.value:\n",
    "    selected_features = train_features_scaled\n",
    "    selected_labels = train_labels\n",
    "    print('Train set selected')\n",
    "elif test_with_dev.value:\n",
    "    selected_features = train_dev_features_scaled\n",
    "    selected_labels = train_dev_labels\n",
    "    print('Development set selected')\n",
    "elif test_with_test.value:\n",
    "    selected_features = test_features_scaled\n",
    "    selected_labels = test_labels\n",
    "    print('Test set selected')\n",
    "else:\n",
    "    selected_features = all_features_scaled\n",
    "    selected_labels = all_labels\n",
    "    print('Full dataset selected, size: {}'.format(len(selected_features)))\n",
    "    print('WARNING - ANY REPORT WILL TAKE A LONG TIME TO COMPLETE DUE TO THE SIZE OF THE DATASET SELECTED')\n",
    "\n",
    "df = pd.DataFrame(selected_features, columns=formatted_dictionary)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Report\n",
    "Run this cell to create the model report."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model tests...\n",
      "KNeighborsClassifier testing model...\n",
      "MLPClassifier testing model...\n"
     ]
    }
   ],
   "source": [
    "models_report(selected_features, selected_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC Plots\n",
    "Run this cell to create the ROC plots for all models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "roc_curve_report(selected_features, selected_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SHAP Plots\n",
    "**---WARNING---**\n",
    "\n",
    "This report takes a considerable amount of time to run!\n",
    "\n",
    "Run this cell to create the SHAP Plots for the top four models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_report(selected_features, selected_labels, lr_classifier)\n",
    "shap_report(selected_features, selected_labels, mpnn_classifier)\n",
    "shap_report(selected_features, selected_labels, xgboost)\n",
    "shap_report(selected_features, selected_labels, kn_classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}