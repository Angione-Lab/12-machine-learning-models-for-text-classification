{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Library imports\n",
    "Import all the library's required for this notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from IPython.core.display import display\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Python Version\n",
    "Check Your Python version before running this notebook.\n",
    "- Python 3.6.X is required to run this notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class PythonVersionException(Exception):\n",
    "    print('Please use Python version 3.6.x')\n",
    "    pass;\n",
    "\n",
    "\n",
    "if re.match('3.6*', sys.version.split('(')[0]) is None:\n",
    "    raise PythonVersionException\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK Corpus Sets\n",
    "Run this section to check if the following corpus datasets have been downloaded, if they are missing this will download\n",
    "them for you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltkDataDir = '../data/nltk_data'\n",
    "\n",
    "nltk.data.path.append(os.path.abspath(nltkDataDir))\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('stopwords', nltkDataDir)\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"testing\")\n",
    "except(LookupError, OSError) as e:\n",
    "    nltk.download('punkt', nltkDataDir)\n",
    "    nltk.download('wordnet', nltkDataDir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "remove_common_artifacts = False\n",
    "verbose_logs = False\n",
    "\n",
    "dictionary = []\n",
    "formatted_dictionary = []\n",
    "selected_set = None\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "train_dev_features = []\n",
    "train_dev_labels = []\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "all_features_scaled = 0\n",
    "train_features_scaled = 0\n",
    "train_dev_features_scaled = 0\n",
    "test_features_scaled = 0\n",
    "\n",
    "max_iterations = 10000\n",
    "\n",
    "kn_classifier = KNeighborsClassifier(algorithm='brute')\n",
    "mpnn_classifier = MLPClassifier(hidden_layer_sizes=50, solver='lbfgs', max_iter=5000)\n",
    "lr_classifier = LogisticRegression(solver='lbfgs', max_iter=max_iterations)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "xgboost = XGBClassifier()\n",
    "mnb_classifier = MultinomialNB()\n",
    "gnb_classifier = GaussianNB()\n",
    "bnb_classifier = BernoulliNB()\n",
    "linear_svc = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', shrinking=True, probability=True,\n",
    "                     tol=0.001, cache_size=1000, max_iter=max_iterations, decision_function_shape='ovr')\n",
    "\n",
    "models = [kn_classifier,\n",
    "          mpnn_classifier,\n",
    "          lr_classifier,\n",
    "          rf_classifier,\n",
    "          xgboost,\n",
    "          mnb_classifier,\n",
    "          gnb_classifier,\n",
    "          bnb_classifier,\n",
    "          linear_svc,\n",
    "          ]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "scores = []\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "scoring_parse_labels = ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "report_labels = ['model_name', 'fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro', 'test_f1']\n",
    "\n",
    "\n",
    "def enron_selector(enable_preprocess, save_files):  # function to identify platform and selected dataset to be applied.\n",
    "    print('Loading enron corpus')\n",
    "\n",
    "    def init_lists(folder_collection, label):  # function to retrieve and apply email content to array.\n",
    "        a_list = []\n",
    "        doc_id = 0\n",
    "        file_list = []\n",
    "        label = \"Loading \" + label + \"...\"\n",
    "        for entry in folder_collection:\n",
    "            b_list = os.listdir(entry)\n",
    "            for item in b_list:\n",
    "                file_list.append(entry + item)\n",
    "        for a_file in file_list:\n",
    "            f = open(a_file, 'r')\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, file_list, label)\n",
    "            try:\n",
    "                a_list.append(f.read())\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            doc_id += 1\n",
    "        f.close()\n",
    "        return a_list\n",
    "\n",
    "    enron_ = ['Enron1/', 'Enron2/', 'Enron3/', 'Enron4/', 'Enron5/', 'Enron6/']\n",
    "    spam = []\n",
    "    ham = []\n",
    "\n",
    "    for i, sub in enumerate(enron_):\n",
    "        spam.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'spam/')\n",
    "        ham.append('../data/enron_dataset/Enron/Processed/' + enron_[i] + 'ham/')\n",
    "\n",
    "    spam = init_lists(spam, \"spam\")\n",
    "    ham = init_lists(ham, \"ham\")\n",
    "\n",
    "    if enable_preprocess:\n",
    "        all_emails = [(email, 'spam') for email in spam]\n",
    "        all_emails += [(email, 'ham') for email in ham]\n",
    "        ham_emails, spam_emails = preprocess(all_emails)\n",
    "    else:\n",
    "        ham_emails = [(email, 'ham') for email in ham]\n",
    "        spam_emails = [(email, 'spam') for email in spam]\n",
    "\n",
    "    if save_files:\n",
    "        if remove_common_artifacts:\n",
    "            ham_file, spam_file = \"../data/processed_ham_remove_common_artifacts.txt\", \"../data/processed_spam_remove_common_artifacts.txt\"\n",
    "        else:\n",
    "            ham_file, spam_file = \"../data/processed_ham.txt\", \"../data/processed_spam.txt\"\n",
    "\n",
    "        print(\"Writing ham file...\")\n",
    "        with open(ham_file, 'w') as fp:\n",
    "            fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in ham_emails))\n",
    "        print(\"Writing spam file...\")\n",
    "        with open(spam_file, 'w') as fp:\n",
    "            fp.write('\\n'.join('{} {};'.format(x[0], x[1]) for x in spam_emails))\n",
    "    return ham_emails, spam_emails\n",
    "\n",
    "\n",
    "def html_list():\n",
    "    html_tag_list = []\n",
    "    location = '../data/html_tag_list.txt'\n",
    "    f = open(location, 'r')\n",
    "    for i in f:\n",
    "        html_tag_list.append(i.strip())\n",
    "    f.close()\n",
    "    return html_tag_list\n",
    "\n",
    "\n",
    "def preprocess(collection):  # function to apply pre-processing: stop words, lemmatise.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print('Pre-processing emails...')\n",
    "    entry_id = 0\n",
    "    doc_id = 0\n",
    "    processed = []\n",
    "    html_tags = [html_list()]\n",
    "    common_artifacts = ['enron', 'subject', 'mail', 'cc', '``', 'email', '\\n', 'www', 'com', '\\nsubject']\n",
    "    for entry in collection:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, collection, 'Pre-processing emails...')\n",
    "        for i, line in enumerate(entry):\n",
    "            emails = ''\n",
    "            if i == 0:\n",
    "                for word in word_tokenize(line):\n",
    "                    item = lemmatizer.lemmatize(word.lower())\n",
    "                    if not item in stoplist:\n",
    "                        if word.isalnum() == False or word in html_tags:\n",
    "                            pass\n",
    "                        elif remove_common_artifacts == True and item in common_artifacts:\n",
    "                            pass\n",
    "                        else:\n",
    "                            emails = emails + item + ','\n",
    "                processed.append(tuple((emails, entry[1])))\n",
    "                del emails\n",
    "                entry_id += 1\n",
    "        doc_id += 1\n",
    "    email_list = []\n",
    "    ham = []\n",
    "    spam = []\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'ham':\n",
    "            email_list.append(entry)\n",
    "            ham.append(entry)\n",
    "    for entry in processed:\n",
    "        if entry[1] == 'spam':\n",
    "            email_list.append(entry)\n",
    "            spam.append(entry)\n",
    "    return ham, spam\n",
    "\n",
    "\n",
    "def dictionary_build(all_emails, preprocess):\n",
    "    print(\"Building Dictionary...\")\n",
    "\n",
    "    html_tags = [html_list()]\n",
    "    common_artifacts = ['enron', 'subject', 'mail', 'cc', '``', 'email', '\\n', 'www', 'com', '\\nsubject']\n",
    "    all_words = []\n",
    "    for entry in all_emails:\n",
    "        for sentence in entry:\n",
    "            if not sentence == 'ham' or sentence == 'spam':\n",
    "                words = str(sentence).split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(word)\n",
    "    dict = Counter(all_words)\n",
    "    if preprocess:\n",
    "        list_to_remove = list(dict)\n",
    "        for item in list_to_remove:\n",
    "            if len(item) <= 1:\n",
    "                del dict[item]\n",
    "            elif item in html_tags:\n",
    "                del dict[item]\n",
    "            elif str(item).isdigit():\n",
    "                del dict[item]\n",
    "            elif remove_common_artifacts == True and item in common_artifacts:\n",
    "                del dict[item]\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "# function to extract features to matrix based on calculating occurrence of words based\n",
    "def extract_features(data, label):\n",
    "    # on dictionary.\n",
    "    features_matrix = np.zeros((len(data), len(dictionary)))\n",
    "    print('Feature extraction \\'' + label + '\\':')\n",
    "    doc_id = 0\n",
    "    all_words = []\n",
    "    for entry in data:\n",
    "        if verbose_logs:\n",
    "            process_status(doc_id, data, label)\n",
    "        for i, line in enumerate(entry):\n",
    "            if i == 0:\n",
    "                # print('[' + str(doc_id) + '] ', entry)\n",
    "                words = line.split(',')\n",
    "                for word in words:\n",
    "                    all_words.append(words)\n",
    "                    for j, d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            word_id = j\n",
    "                            features_matrix[doc_id, word_id] = words.count(word)\n",
    "        doc_id = doc_id + 1\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calculate(ham, spam):\n",
    "    main_proportion = 0.8\n",
    "    ham_size = int(len(ham) * main_proportion)\n",
    "    ham_train, ham_test = ham[:ham_size], ham[ham_size:]\n",
    "    spam_size = int(len(spam) * main_proportion)\n",
    "    spam_train, spam_test = spam[:spam_size], spam[spam_size:]\n",
    "    ham_train_size, spam_train_size = int(len(ham_train) * main_proportion), int(len(spam_train) * main_proportion)\n",
    "    ham_train, ham_train_dev = ham_train[:ham_train_size], ham_train[ham_train_size:]\n",
    "    spam_train, spam_train_dev = spam_train[:spam_train_size], spam_train[spam_train_size:]\n",
    "    train_set, train_dev_set, test_set = ham_train + spam_train, ham_train_dev + spam_train_dev, ham_test + spam_test\n",
    "    train_labels = np.zeros(len(train_set))\n",
    "    train_labels[(int((len(train_set)) - len(spam_train))):len(train_set)] = 1\n",
    "    train_dev_labels = np.zeros(len(train_dev_set))\n",
    "    train_dev_labels[(int((len(train_dev_set)) - len(spam_train_dev))):len(train_dev_set)] = 1\n",
    "    test_labels = np.zeros(len(test_set))\n",
    "    test_labels[(int((len(test_set)) - len(spam_test))):len(test_set)] = 1\n",
    "\n",
    "    all_set = ham_train + ham_train_dev + ham_test + spam_train + spam_train_dev + spam_test\n",
    "    all_labels = np.zeros(len(all_set))\n",
    "    all_labels[(int((len(all_labels)) - len(spam_train + spam_train_dev + spam_test))): len(all_set)] = 1\n",
    "\n",
    "    print(\"Train set:\\n\", \"Ham: \", str(len(ham_train)), \"\\n\", \"Spam: \", str(len(spam_train)),\n",
    "          \"\\nTrain_Dev:\\n Ham:\", str(len(ham_train_dev)), \"\\n Spam:\", str(len(spam_train_dev)),\n",
    "          \"\\nTest set:\\n\", \"Ham: \", str(len(ham_test)), \"\\n\", \"Spam: \", str(len(spam_test)),\n",
    "          \"\\nAll set:\\n\", \"Ham: \", str(len(ham_train + ham_train_dev + ham_test)), \"\\n\", \"Spam: \",\n",
    "          str(len(spam_train + spam_train_dev + spam_test)))\n",
    "    return train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels, all_set, all_labels\n",
    "\n",
    "\n",
    "def load_(file, label):\n",
    "    if verbose_logs == False:\n",
    "        print('Loading ' + label + ' dataset')\n",
    "    with open(file, 'r') as fp:\n",
    "        values = []\n",
    "        doc_id = 0\n",
    "        size_file = fp.read().split(\";\")\n",
    "        for item in size_file:\n",
    "            if verbose_logs:\n",
    "                process_status(doc_id, size_file, \"loading \" + label + \" file...\")\n",
    "            values.append(item.split(\", \"))\n",
    "            doc_id += 1\n",
    "        return values\n",
    "\n",
    "\n",
    "def process_status(id, data, label):\n",
    "    if id + 1 < int(len(data)):\n",
    "        end_atp = \"\\r\"\n",
    "    elif id + 1 <= int(len(data)):\n",
    "        end_atp = \"\\n\"\n",
    "    return print(label, '%0.0i out of %0.0i: %0.0i' %\n",
    "                 (id + 1, len(data), int((id + 1) * (100 / len(data)))), '%', end='\\r', flush=True)\n",
    "\n",
    "\n",
    "def determine_model_name(model):\n",
    "    model_name = type(model).__name__\n",
    "    if model_name.lower() == 'svc':\n",
    "        return model_name + '_' + model.kernel\n",
    "    else:\n",
    "        return type(model).__name__\n",
    "\n",
    "\n",
    "def handle_scores_to_csv(file_name, save):\n",
    "    processed_scores = []\n",
    "    for i, d in enumerate(scores):\n",
    "        processed_score = [d[0]]\n",
    "        print(d)\n",
    "        for c, e in enumerate(scoring_parse_labels):\n",
    "            item = d[1][e]\n",
    "            item = item.astype(np.float)\n",
    "            if scoring_parse_labels[c] == 'fit_time' or scoring_parse_labels[c] == 'score_time':\n",
    "                processed_score.append('%0.6f' % (np.mean(item)))\n",
    "            else:\n",
    "                processed_score.append('%0.2f' % (float(np.mean(item))))\n",
    "        processed_scores.append(processed_score)\n",
    "\n",
    "    df = pd.DataFrame(processed_scores, columns=report_labels)\n",
    "    if save:\n",
    "        df.to_csv(file_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def roc_curve_report(X, y):\n",
    "    for a in range(len(models)):\n",
    "        model = models[a]\n",
    "        model_name = determine_model_name(model)\n",
    "        try:\n",
    "            tprs = []\n",
    "            aucs = []\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "            i = 0\n",
    "            fig, ax = plt.subplots()\n",
    "            for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "                model.fit(X[train], y[train])\n",
    "                viz = plot_roc_curve(model, X[test], y[test],\n",
    "                                     name='ROC fold {}'.format(i),\n",
    "                                     alpha=0.3, lw=1, ax=ax)\n",
    "                interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "                interp_tpr[0] = 0.0\n",
    "                tprs.append(interp_tpr)\n",
    "                aucs.append(viz.roc_auc)\n",
    "\n",
    "            ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "                    label='Chance', alpha=.8)\n",
    "\n",
    "            mean_tpr = np.mean(tprs, axis=0)\n",
    "            mean_tpr[-1] = 1.0\n",
    "            mean_auc = auc(mean_fpr, mean_tpr)\n",
    "            std_auc = np.std(aucs)\n",
    "            ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "                    label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                    lw=2, alpha=.8)\n",
    "\n",
    "            std_tpr = np.std(tprs, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                            label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "            ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "                   title=model_name)\n",
    "            ax.legend(loc=\"lower right\")\n",
    "            plt.savefig(\n",
    "                \"../results/plots/test/roc_%s_%0i_features_%0i_test.pdf\" % (\n",
    "                    model_name, feature_size, len(X)),\n",
    "                dpi=100, facecolor='w', edgecolor='b', orientation='portrait', transparent=False, bbox_inches=None,\n",
    "                pad_inches=0.1)\n",
    "            print(\"Created %s ROC figure\" % model_name)\n",
    "            plt.close()\n",
    "        except (AttributeError, OverflowError) as detail:\n",
    "            print(model_name + \" Failed due to \", detail)\n",
    "\n",
    "\n",
    "def shap_report(X, y, model):\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y)\n",
    "\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X, y.values.ravel(), random_state=1)\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    med = train_X.mean().values.reshape((1, train_X.shape[1]))\n",
    "\n",
    "    # Create object that can calculate shap values\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, med)\n",
    "    # Calculate Shap values\n",
    "    shap_values = explainer.shap_values(val_X.iloc[0:len(train_X), :], nsamples=len(train_X))\n",
    "\n",
    "    shap.summary_plot(shap_values[1], val_X.iloc[0:len(train_X), :], formatted_dictionary, show=False, color_bar=True,\n",
    "                      max_display=10,\n",
    "                      plot_size=(11, 8))\n",
    "    report_file_name = str(\n",
    "        len(dictionary)) + '_features_' + determine_model_name(\n",
    "        model) + '_shap_plot_beeswarm_' + datetime.now().strftime(\n",
    "        '%Y-%m-%dT%H-%M-%S%z') + '_.pdf'\n",
    "\n",
    "    plt.savefig('../results/shap/' + report_file_name)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# function to test and record via csv, all algorithms selected.\n",
    "def models_report(X, y, save):\n",
    "    if len(scores) > 0:\n",
    "        scores.clear()\n",
    "\n",
    "    cv_collection = []\n",
    "    for i in range(6):\n",
    "        cv_t = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        cv_collection.append(cv_t)\n",
    "\n",
    "    print('Processing model tests...')\n",
    "    performance_report = pd.DataFrame(columns=report_labels)\n",
    "\n",
    "    for a in range(len(models)):\n",
    "        model = models[a]\n",
    "        model_name = determine_model_name(model)\n",
    "        print('{} testing model...'.format(model_name))\n",
    "        for count in range(len(cv_collection)):\n",
    "            print('batch: {} | model: {}, processing...'.format(count + 1, model_name))\n",
    "            score = cross_validate(model, X, y, cv=cv_collection[count], scoring=scoring, return_train_score=False)\n",
    "            for i in range(5):\n",
    "                values = [model_name]\n",
    "                for key in score.keys():\n",
    "                    values.append(score[key][i])\n",
    "                performance_report = performance_report.append(dict(zip(performance_report.columns, values)),\n",
    "                                                               ignore_index=True)\n",
    "\n",
    "    report_file_name = str(len(dictionary)) + '_features' '_models_test_report_' + datetime.now().strftime(\n",
    "        '%Y-%m-%dT%H-%M-%S%z') + '.csv'\n",
    "\n",
    "    if save:\n",
    "        performance_report.to_csv('../results/scores/' + report_file_name)\n",
    "    else:\n",
    "        performance_report.head()\n",
    "\n",
    "    print('Models test finished.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# User Input\n",
    "Please select the following options to for the notebook to process, please run the cell below to render the widgets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, GridspecLayout\n",
    "\n",
    "defined_widgets = []\n",
    "\n",
    "preprocess_label = widgets.Label('Preprocessing and Feature Extraction Stage:')\n",
    "defined_widgets.append(preprocess_label)\n",
    "\n",
    "feature_size_widget = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=50,\n",
    "    max=1000,\n",
    "    description='Feature size'\n",
    ")\n",
    "defined_widgets.append(feature_size_widget)\n",
    "\n",
    "show_logs = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show logs during process',\n",
    ")\n",
    "defined_widgets.append(show_logs)\n",
    "\n",
    "enable_preprocess_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enable preprocessing data.',\n",
    ")\n",
    "defined_widgets.append(enable_preprocess_widget)\n",
    "\n",
    "exclude_common_artifacts = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Exclude common artifacts',\n",
    ")\n",
    "defined_widgets.append(exclude_common_artifacts)\n",
    "\n",
    "reprocess_saved_feature_set = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Use processed feature dataset',\n",
    ")\n",
    "defined_widgets.append(reprocess_saved_feature_set)\n",
    "\n",
    "test_label_widget = widgets.Label('Set selection')\n",
    "defined_widgets.append(test_label_widget)\n",
    "\n",
    "test_with_all = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Full Dataset',\n",
    ")\n",
    "defined_widgets.append(test_with_all)\n",
    "\n",
    "test_with_train = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Train Dataset',\n",
    ")\n",
    "defined_widgets.append(test_with_train)\n",
    "\n",
    "test_with_dev = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Development Dataset',\n",
    ")\n",
    "defined_widgets.append(test_with_dev)\n",
    "\n",
    "test_with_test = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Test Dataset',\n",
    ")\n",
    "defined_widgets.append(test_with_test)\n",
    "\n",
    "grid = GridspecLayout(len(defined_widgets), 1)\n",
    "\n",
    "for i in range(len(defined_widgets)):\n",
    "    grid[i, 0] = defined_widgets[i]\n",
    "\n",
    "display(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    feature_size = feature_size_widget.value\n",
    "    verbose_logs = show_logs.value\n",
    "    remove_common_artifacts = exclude_common_artifacts.value\n",
    "    scores = []\n",
    "    if remove_common_artifacts:\n",
    "        a_files = [\"../data/processed_ham_remove_common_artifacts.txt\",\n",
    "                   \"../data/processed_spam_remove_common_artifacts.txt\"]\n",
    "    else:\n",
    "        a_files = [\"../data/processed_ham.txt\", \"../data/processed_spam.txt\"]\n",
    "\n",
    "    if reprocess_saved_feature_set.value:\n",
    "        ham_collection = load_(a_files[0], \"ham\")\n",
    "        spam_collection = load_(a_files[1], \"spam\")\n",
    "    else:\n",
    "        ham_collection, spam_collection = enron_selector(enable_preprocess_widget.value, True)\n",
    "    dictionary = dictionary_build((ham_collection + spam_collection), enable_preprocess_widget.value)\n",
    "    dictionary = dictionary.most_common(feature_size)\n",
    "    formatted_dictionary = []\n",
    "    for item in dictionary:\n",
    "        formatted_dictionary.append(item[0])\n",
    "\n",
    "    print(dictionary)\n",
    "    train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels, all_set, all_labels = calculate(\n",
    "        ham_collection, spam_collection)\n",
    "\n",
    "    if test_with_all.value:\n",
    "        all_features = extract_features(all_set, 'all_set')\n",
    "    elif test_with_train.value:\n",
    "        train_features = extract_features(train_set, \"train\")\n",
    "        selected_set = 'train'\n",
    "    elif test_with_dev.value:\n",
    "        train_dev_features = extract_features(train_dev_set, \"train_dev\")\n",
    "        selected_set = 'dev'\n",
    "    elif test_with_test.value:\n",
    "        test_features = extract_features(test_set, \"test\")\n",
    "        selected_set = 'test'\n",
    "\n",
    "    if isinstance(all_features, np.ndarray):\n",
    "        all_features_scaled = MinMaxScaler().fit_transform(all_features, all_labels)\n",
    "\n",
    "    if isinstance(train_features, np.ndarray):\n",
    "        train_features_scaled = MinMaxScaler().fit_transform(train_features, train_labels)\n",
    "\n",
    "    if isinstance(train_dev_features, np.ndarray):\n",
    "        train_dev_features_scaled = MinMaxScaler().fit_transform(train_dev_features, train_dev_labels)\n",
    "\n",
    "    if isinstance(test_features, np.ndarray):\n",
    "        test_features_scaled = MinMaxScaler().fit_transform(test_features, test_labels)\n",
    "\n",
    "    print('Finished feature extraction')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Selection\n",
    "Please make sure the last cell have completed the run successfully.\n",
    "\n",
    "Run this cell to determine which feature set you've selected for reporting, refer to the user input cell on which set\n",
    "was selected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if test_with_train.value:\n",
    "    selected_features = train_features_scaled\n",
    "    selected_labels = train_labels\n",
    "    print('Train set selected')\n",
    "elif test_with_dev.value:\n",
    "    selected_features = train_dev_features_scaled\n",
    "    selected_labels = train_dev_labels\n",
    "    print('Development set selected')\n",
    "elif test_with_test.value:\n",
    "    selected_features = test_features_scaled\n",
    "    selected_labels = test_labels\n",
    "    print('Test set selected')\n",
    "else:\n",
    "    selected_features = all_features_scaled\n",
    "    selected_labels = all_labels\n",
    "    print('Full dataset selected, size: {}'.format(len(selected_features)))\n",
    "    print('WARNING - ANY REPORT WILL TAKE A LONG TIME TO COMPLETE DUE TO THE SIZE OF THE DATASET SELECTED')\n",
    "\n",
    "df = pd.DataFrame(selected_features, columns=formatted_dictionary)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Report\n",
    "Run this cell to create the model report."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_report(selected_features, selected_labels, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC Plots\n",
    "Run this cell to create the ROC plots for all models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "\n",
    "def roc_curve_report1(X, y):\n",
    "    for a in range(len(models)):\n",
    "        model = models[a]\n",
    "        model_name = determine_model_name(model)\n",
    "        try:\n",
    "            tprs = []\n",
    "            aucs = []\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "            i = 0\n",
    "            fig, ax = plt.subplots()\n",
    "            for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "                model.fit(X[train], y[train])\n",
    "                viz = plot_roc_curve(model, X[test], y[test],\n",
    "                                     name='ROC fold {}'.format(i),\n",
    "                                     alpha=0.3, lw=1, ax=ax)\n",
    "                interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "                interp_tpr[0] = 0.0\n",
    "                tprs.append(interp_tpr)\n",
    "                aucs.append(viz.roc_auc)\n",
    "\n",
    "            ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "                    label='Chance', alpha=.8)\n",
    "\n",
    "            mean_tpr = np.mean(tprs, axis=0)\n",
    "            mean_tpr[-1] = 1.0\n",
    "            mean_auc = auc(mean_fpr, mean_tpr)\n",
    "            std_auc = np.std(aucs)\n",
    "            ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "                    label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                    lw=2, alpha=.8)\n",
    "\n",
    "            std_tpr = np.std(tprs, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                            label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "            ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "                   title=model_name)\n",
    "            ax.legend(loc=\"lower right\")\n",
    "            plt.savefig(\n",
    "                \"../results/plots/test/roc_%s_%0i_features_%0i_test.pdf\" % (\n",
    "                    model_name, feature_size, len(X)),\n",
    "                dpi=100, facecolor='w', edgecolor='b', orientation='portrait', transparent=False, bbox_inches=None,\n",
    "                pad_inches=0.1)\n",
    "            print(\"Created %s ROC figure\" % model_name)\n",
    "            plt.close()\n",
    "        except (AttributeError, OverflowError) as detail:\n",
    "            print(model_name + \" Failed due to \", detail)\n",
    "\n",
    "\n",
    "roc_curve_report1(selected_features, selected_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SHAP Plots\n",
    "**---WARNING---**\n",
    "\n",
    "This report takes a considerable amount of time to run!\n",
    "\n",
    "Run this cell to create the SHAP Plots for the top four models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_report(selected_features, selected_labels, lr_classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_report(selected_features, selected_labels, mpnn_classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_report(selected_features, selected_labels, xgboost)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_report(selected_features, selected_labels, kn_classifier)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## T_test figures report\n",
    "The following cell creates the report for all the figures to be used in the T_test implementation in R."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def t_test_results(X, y, iterations):  # function to test and record via csv, all algorithms selected.\n",
    "    cv_collection = []\n",
    "\n",
    "    # Ensure all cv collections are the same for each iteration of each model since shuffle is set to true.\n",
    "    for i in range(iterations):\n",
    "        cv_t = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        cv_collection.append(cv_t)\n",
    "\n",
    "    print('Processing features test...')\n",
    "\n",
    "    for a in range(len(models)):\n",
    "        model = models[a]\n",
    "        model_name = type(model).__name__\n",
    "        if model_name.lower() == 'svc':\n",
    "            model_name_field = model_name + '_' + model.kernel\n",
    "        else:\n",
    "            model_name_field = type(model).__name__\n",
    "\n",
    "        model_score = []\n",
    "        for count in range(iterations):\n",
    "            print('batch: {} | model: {}, processing...'.format(count + 1, model_name_field))\n",
    "            score = cross_validate(models[a], X, y, cv=cv_collection[count], scoring='f1', return_train_score=False)\n",
    "            if len(model_score) != 0:\n",
    "                model_score = np.append(model_score, score['test_score'])\n",
    "            else:\n",
    "                model_score.append(score['test_score'])\n",
    "\n",
    "        scores.append([model_name_field, ','.join(str(v) for v in model_score)])\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=['model_name', 'f1_score'])\n",
    "    df.to_csv('../results/scores/t_test_values.csv')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "t_test_results(selected_features, selected_labels, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature selection test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_size_test():\n",
    "    feature_sizes = [10, 25, 50, 75, 100, 125, 150, 200]\n",
    "    df_results = []\n",
    "\n",
    "    for size in feature_sizes:\n",
    "        if len(scores) > 0:\n",
    "            scores.clear()\n",
    "\n",
    "        dictionary = dictionary_build((ham_collection + spam_collection), enable_preprocess_widget.value)\n",
    "        dictionary = dictionary.most_common(size)\n",
    "        formatted_dictionary = []\n",
    "        for item in dictionary:\n",
    "            formatted_dictionary.append(item[0])\n",
    "\n",
    "        print(size)\n",
    "        print(dictionary)\n",
    "\n",
    "        train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels, all_set, all_labels = calculate(\n",
    "            ham_collection, spam_collection)\n",
    "\n",
    "        test_features = extract_features(test_set, \"test\")\n",
    "\n",
    "        test_features_scaled = MinMaxScaler().fit_transform(test_features, test_labels)\n",
    "\n",
    "        df_results.append(models_report(test_features_scaled, test_labels, False))\n",
    "\n",
    "    pd.DataFrame().append([df_results[0],\n",
    "                           df_results[1],\n",
    "                           df_results[2],\n",
    "                           df_results[3],\n",
    "                           df_results[4],\n",
    "                           df_results[5],\n",
    "                           df_results[6],\n",
    "                           df_results[7]]).to_csv('../results/scores/feature_selection_test_report.csv')\n",
    "\n",
    "\n",
    "feature_size_test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Optimisation Tests\n",
    "The following cells below contain the tests used to optimise some models used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-Nearest Neighbours"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kn_test(X, y):\n",
    "    if len(scores) > 0:\n",
    "        scores.clear()\n",
    "\n",
    "    cv_collection = []\n",
    "\n",
    "    # Ensure all cv collections are the same for each iteration of each model since shuffle is set to true.\n",
    "    for i in range(6):\n",
    "        cv_t = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        cv_collection.append(cv_t)\n",
    "\n",
    "    algorithm_val = ['ball_tree', 'kd_tree', 'brute']\n",
    "    performance_report = pd.DataFrame(columns=report_labels)\n",
    "\n",
    "    for a in range(len(algorithm_val)):\n",
    "        model = KNeighborsClassifier(algorithm=algorithm_val[a])\n",
    "        model_name = type(model).__name__ + '_' + model.algorithm\n",
    "\n",
    "        for count in range(len(cv_collection)):\n",
    "            print('batch: {} | model: {}, processing...'.format(count + 1, model_name))\n",
    "            score = cross_validate(model, X, y, cv=cv_collection[count], scoring=scoring, return_train_score=False)\n",
    "            for i in range(5):\n",
    "                values = [model.algorithm]\n",
    "                for key in score.keys():\n",
    "                    values.append(score[key][i])\n",
    "                performance_report = performance_report.append(dict(zip(performance_report.columns, values)),\n",
    "                                                               ignore_index=True)\n",
    "\n",
    "    performance_report.to_csv('../results/scores/k_neighbour_classifier_test_score.csv')\n",
    "\n",
    "\n",
    "kn_test(selected_features, selected_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multilayer Perceptron Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Multi-layer perceptron neural network test.\n",
    "def mpnn_test(X, y):\n",
    "    nu_val = [25, 50, 75, 100, 200]\n",
    "    h_layers = [1, 2, 3, 4, 5]\n",
    "    iter_val = [2000, 3500, 5000, 7500, 10000]\n",
    "    i = 1\n",
    "\n",
    "    for a in range(len(nu_val)):\n",
    "        for b in range(len(h_layers)):\n",
    "            if b > 0:\n",
    "                if b == 1:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a]\n",
    "                elif b == 2:\n",
    "                    nu_layer_val = nu_val[a], nu_val[a], nu_val[a]\n",
    "            else:\n",
    "                nu_layer_val = nu_val[a]\n",
    "            for c in range(len(iter_val)):\n",
    "                print('{} of total {}'.format(i, len(nu_val) * len(h_layers) * len(iter_val)))\n",
    "                model = MLPClassifier(hidden_layer_sizes=(nu_layer_val), solver='lbfgs', max_iter=iter_val[c])\n",
    "                scores.append(['{}_{}_{}_{}'.format(determine_model_name(model), nu_val[a], h_layers[b], iter_val[c]),\n",
    "                               cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False)])\n",
    "                i += 1\n",
    "\n",
    "    handle_scores_to_csv('../results/scores/mpnn_parameter_test_scores.csv', True)\n",
    "\n",
    "\n",
    "mpnn_test(selected_features, selected_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def svm_tests(X, y):\n",
    "    if len(scores) > 0:\n",
    "        scores.clear()\n",
    "\n",
    "    cv_collection = []\n",
    "\n",
    "    # Ensure all cv collections are the same for each iteration of each model since shuffle is set to true.\n",
    "    for i in range(6):\n",
    "        cv_t = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        cv_collection.append(cv_t)\n",
    "\n",
    "    kernel_val = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    performance_report = pd.DataFrame(columns=report_labels)\n",
    "\n",
    "    for a in range(len(kernel_val)):\n",
    "        model = svm.SVC(C=1.0, kernel=kernel_val[a], degree=3, gamma='auto', coef0=0.0, shrinking=True,\n",
    "                        probability=True, tol=0.001, cache_size=10000, class_weight=None, verbose=False,\n",
    "                        max_iter=max_iterations, decision_function_shape='ovr', random_state=None)\n",
    "        model_name = type(model).__name__ + '_' + model.kernel\n",
    "\n",
    "        for count in range(len(cv_collection)):\n",
    "            print('batch: {} | model: {}, processing...'.format(count + 1, model_name))\n",
    "            score = cross_validate(model, X, y, cv=cv_collection[count], scoring=scoring, return_train_score=False)\n",
    "            for i in range(5):\n",
    "                values = [model.kernel]\n",
    "                for key in score.keys():\n",
    "                    values.append(score[key][i])\n",
    "                performance_report = performance_report.append(dict(zip(performance_report.columns, values)),\n",
    "                                                               ignore_index=True)\n",
    "\n",
    "    performance_report.to_csv('../results/scores/svm_kernel_test_scores.csv')\n",
    "\n",
    "\n",
    "svm_tests(selected_features, selected_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Preprocessing Comparison Test\n",
    "The following test creates the comparison plot between preprocessed vs raw datsets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_test(processed_score, non_processed_score, model_labels):\n",
    "    n_groups = len(model_labels)\n",
    "\n",
    "    # create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, processed_score, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='m',\n",
    "                     label='Processed')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, non_processed_score, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     label='Raw')\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Score difference between preprocessed vs raw dataset')\n",
    "    plt.xticks(index + bar_width, model_labels)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/preprocessing_score_difference.pdf', dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "remove_common_artifacts = True\n",
    "ham_collection_preprocessed, spam_collection_preprocessed = enron_selector(True, False)\n",
    "remove_common_artifacts = False\n",
    "ham_collection_no_processing, spam_collection_no_processing = enron_selector(False, False)\n",
    "remove_common_artifacts = exclude_common_artifacts.value\n",
    "\n",
    "ham_list = [ham_collection_preprocessed, ham_collection_no_processing]\n",
    "spam_list = [spam_collection_preprocessed, spam_collection_no_processing]\n",
    "dfs = []\n",
    "\n",
    "for i in range(len(ham_list)):\n",
    "    scores = []\n",
    "    dictionary = dictionary_build((ham_list[i] + spam_list[i]), enable_preprocess_widget.value)\n",
    "    dictionary = dictionary.most_common(feature_size)\n",
    "    formatted_dictionary = []\n",
    "    for item in dictionary:\n",
    "        formatted_dictionary.append(item[0])\n",
    "\n",
    "    print(dictionary)\n",
    "\n",
    "    train_set, test_set, train_labels, test_labels, train_dev_set, train_dev_labels, all_set, all_labels = calculate(\n",
    "        ham_list[i], spam_list[i])\n",
    "\n",
    "    test_features = extract_features(test_set, \"test\")\n",
    "    test_features_scaled = MinMaxScaler().fit_transform(test_features, test_labels)\n",
    "\n",
    "    print('Finished feature extraction')\n",
    "\n",
    "    dfs.append(models_report(test_features_scaled, test_labels))\n",
    "\n",
    "dfs[0]['test_f1'].to_csv('../results/scores/comparison_processed_results.csv')\n",
    "dfs[1]['test_f1'].to_csv('../results/scores/comparison_raw_results.csv')\n",
    "\n",
    "# Get only top 4 models for plot.\n",
    "model_select = [1, 4, 7, 8]\n",
    "selected_processed_results = []\n",
    "selected_non_processed_results = []\n",
    "selected_model_names = []\n",
    "\n",
    "for i, num in enumerate(model_select):\n",
    "    if num == 1:\n",
    "        selected_model_names.append('MPNN')\n",
    "    elif num == 4:\n",
    "        selected_model_names.append('XGBOOST')\n",
    "    elif num == 7:\n",
    "        selected_model_names.append('BernoulliNB')\n",
    "    else:\n",
    "        selected_model_names.append('LinearSVC')\n",
    "\n",
    "    selected_processed_results.append(float(dfs[0]['test_f1'].iloc[num]))\n",
    "    selected_non_processed_results.append(float(dfs[1]['test_f1'].iloc[num]))\n",
    "\n",
    "plot_test(selected_processed_results, selected_non_processed_results, selected_model_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}